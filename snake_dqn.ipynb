{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "# Pygame setup\n",
    "pygame.init()\n",
    "# Colors and window dimensions\n",
    "blanc, jaune, noir, rouge, vert, bleu = (255, 255, 255), (255, 255, 102), (0, 0, 0), (213, 50, 80), (0, 255, 0), (50, 153, 213)\n",
    "largeur_ecran, hauteur_ecran, taille_bloc, vitesse_serpent = 600, 400, 20, 40\n",
    "fenetre = pygame.display.set_mode((largeur_ecran, hauteur_ecran))\n",
    "pygame.display.set_caption(\"Snake Game - DQN\")\n",
    "horloge = pygame.time.Clock()\n",
    "police = pygame.font.SysFont(\"bahnschrift\", 25)\n",
    "\n",
    "# Display score\n",
    "def afficher_score(score):\n",
    "    valeur = police.render(\"Score: \" + str(score), True, jaune)\n",
    "    fenetre.blit(valeur, [0, 0])\n",
    "\n",
    "# Neural Network for DQN with two selectable architectures\n",
    "\n",
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, complex_model=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define network architecture based on model complexity\n",
    "        if complex_model:\n",
    "            # Complex model with two hidden layers\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "            self.fc3 = nn.Linear(hidden_size * 2, output_size)\n",
    "            self.complex = True\n",
    "        else:\n",
    "            # Simple model with one hidden layer\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "            self.complex = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Pass through additional layer if complex model\n",
    "        if self.complex:\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)  # Output layer\n",
    "        else:\n",
    "            x = self.fc2(x)  # Output layer for simple model\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        # Save model weights to specified directory\n",
    "        model_folder_path = './models'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "        \n",
    "        file_path = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "# Q-learning Trainer Class\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # Convert inputs to tensors, adding batch dimension if needed\n",
    "        state = torch.FloatTensor(state).unsqueeze(0) if len(state.shape) == 1 else torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0) if len(next_state.shape) == 1 else torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action]) if isinstance(action, int) else torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor([reward]) if isinstance(reward, (int, float)) else torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor([done]) if isinstance(done, bool) else torch.FloatTensor(done)\n",
    "\n",
    "        # Predicted Q values for current state-action pairs\n",
    "        pred = self.model(state).gather(1, action.view(-1, 1))\n",
    "\n",
    "        # Calculate target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_state).max(1)[0]\n",
    "            target_q_values = reward + (1 - done) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(pred, target_q_values.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "# Paramètres pour le DQN (modifiés)\n",
    "HIDDEN_SIZE = 128  # Increased hidden size\n",
    "OUTPUT_SIZE = 4\n",
    "BATCH_SIZE = 64  # Increased batch size\n",
    "MEMORY_SIZE = 200000  # Increased memory size\n",
    "UPDATE_TARGET_EVERY = 4  # Update target network less frequently\n",
    "START_TRAINING_THRESHOLD = BATCH_SIZE * 10\n",
    "MAX_EPISODES = 1000\n",
    "# Agent with model, training, and actions\n",
    "class AIAgent:\n",
    "    def __init__(self, input_size, hidden_size, output_size, gamma=0.99, lr=0.001, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.9, complex_model=False, max_episodes=1000, batch_size=128):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        \n",
    "        # Model initialization\n",
    "        self.model = Linear_QNet(input_size, hidden_size, output_size)\n",
    "        self.target_model = Linear_QNet(input_size, hidden_size, output_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.trainer = QTrainer(self.model, lr=lr, gamma=gamma)\n",
    "        \n",
    "        # Tracking episodes and batch size for training\n",
    "        self.games_played = 0\n",
    "        self.episode_memory = []\n",
    "        self.max_episodes = max_episodes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head = snake[-1]\n",
    "        point_l = [head[0] - taille_bloc, head[1]]\n",
    "        point_r = [head[0] + taille_bloc, head[1]]\n",
    "        point_u = [head[0], head[1] - taille_bloc]\n",
    "        point_d = [head[0], head[1] + taille_bloc]\n",
    "        \n",
    "        dir_l = head[0] > snake[-2][0] if len(snake) > 1 else False\n",
    "        dir_r = head[0] < snake[-2][0] if len(snake) > 1 else False\n",
    "        dir_u = head[1] > snake[-2][1] if len(snake) > 1 else False\n",
    "        dir_d = head[1] < snake[-2][1] if len(snake) > 1 else False\n",
    "\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and self.is_collision(point_r, snake)) or \n",
    "            (dir_l and self.is_collision(point_l, snake)) or \n",
    "            (dir_u and self.is_collision(point_u, snake)) or \n",
    "            (dir_d and self.is_collision(point_d, snake)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and self.is_collision(point_r, snake)) or \n",
    "            (dir_d and self.is_collision(point_l, snake)) or \n",
    "            (dir_l and self.is_collision(point_u, snake)) or \n",
    "            (dir_r and self.is_collision(point_d, snake)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and self.is_collision(point_r, snake)) or \n",
    "            (dir_u and self.is_collision(point_l, snake)) or \n",
    "            (dir_r and self.is_collision(point_u, snake)) or \n",
    "            (dir_l and self.is_collision(point_d, snake)),\n",
    "            \n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "            \n",
    "            # Food location \n",
    "            food[0] < head[0],  # food left\n",
    "            food[0] > head[0],  # food right\n",
    "            food[1] < head[1],  # food up\n",
    "            food[1] > head[1]   # food down\n",
    "        ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def is_collision(self, point, snake):\n",
    "        if point[0] < 0 or point[0] >= largeur_ecran or point[1] < 0 or point[1] >= hauteur_ecran:\n",
    "            return True\n",
    "        if point in snake[:-1]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Train on batch\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = 80 - self.games_played\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            return random.randint(0, 2)\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state_tensor)\n",
    "            return torch.argmax(prediction).item()\n",
    "\n",
    "   \n",
    "    def store_episode(self, state, action, reward, next_state, done):\n",
    "        self.episode_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_on_episode(self):\n",
    "        for experience in self.episode_memory:\n",
    "            self.remember(*experience)\n",
    "        \n",
    "        # Train only if enough memory is stored\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            self.train_long_memory()\n",
    "        \n",
    "        # Clear episode memory after training\n",
    "        self.episode_memory.clear()\n",
    "\n",
    "def jeu():\n",
    "    agent = AIAgent(11, HIDDEN_SIZE, OUTPUT_SIZE, complex_model=True)  \n",
    "    nb_episodes, total_score, high_score = 0, 0, 0\n",
    "    scores, avg_scores = [], []\n",
    "    \n",
    "    while nb_episodes < MAX_EPISODES:\n",
    "        game_over = False\n",
    "        score = 0\n",
    "        serpent = [[largeur_ecran / 2, hauteur_ecran / 2]]\n",
    "        x_nourriture, y_nourriture = round(random.randrange(0, largeur_ecran - taille_bloc) / taille_bloc) * taille_bloc, round(random.randrange(0, hauteur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "        steps_without_food = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # Get the state and decide on an action\n",
    "            state = agent.get_state(serpent, (x_nourriture, y_nourriture))\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Move snake based on action\n",
    "            x_change, y_change = [(0, -taille_bloc), (0, taille_bloc), (-taille_bloc, 0), (taille_bloc, 0)][action]\n",
    "            nouvelle_tete = [serpent[-1][0] + x_change, serpent[-1][1] + y_change]\n",
    "            serpent.append(nouvelle_tete)\n",
    "\n",
    "            # Check for game over\n",
    "            if (nouvelle_tete[0] < 0 or nouvelle_tete[0] >= largeur_ecran or\n",
    "                nouvelle_tete[1] < 0 or nouvelle_tete[1] >= hauteur_ecran or\n",
    "                nouvelle_tete in serpent[:-1]):\n",
    "                game_over = True\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = -0.01  # Small survival reward\n",
    "\n",
    "                # Check for food consumption\n",
    "                if nouvelle_tete[0] == x_nourriture and nouvelle_tete[1] == y_nourriture:\n",
    "                    reward = 50  # Eating food reward\n",
    "                    score += 1\n",
    "                    x_nourriture = round(random.randrange(0, largeur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "                    y_nourriture = round(random.randrange(0, hauteur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "                    steps_without_food = 0\n",
    "                else:\n",
    "                    serpent.pop(0)  # Snake moves without growing\n",
    "\n",
    "                    # Reward adjustment for distance to food\n",
    "                    if len(serpent) > 1:\n",
    "                        distance_before = np.linalg.norm(np.array(serpent[-2]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    else:\n",
    "                        distance_before = np.linalg.norm(np.array(serpent[-1]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    distance_after = np.linalg.norm(np.array(nouvelle_tete) - np.array([x_nourriture, y_nourriture]))\n",
    "                    reward += 10 if distance_after < distance_before else -0.5\n",
    "                    # distance_before = np.linalg.norm(np.array(serpent[-2]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    # distance_after = np.linalg.norm(np.array(nouvelle_tete) - np.array([x_nourriture, y_nourriture]))\n",
    "                    # reward += 10 if distance_after < distance_before else -0.5\n",
    "                    reward += 0.1  # Additional survival reward\n",
    "\n",
    "                    steps_without_food += 1\n",
    "                    if steps_without_food > 100:  # Penalize long survival without eating\n",
    "                        reward -= 1\n",
    "                        steps_without_food = 0\n",
    "\n",
    "            # Prepare for the next step\n",
    "            next_state = agent.get_state(serpent, (x_nourriture, y_nourriture))\n",
    "            agent.store_episode(state, action, reward, next_state, game_over)\n",
    "\n",
    "            # Update Pygame window\n",
    "            fenetre.fill(noir)\n",
    "            pygame.draw.rect(fenetre, rouge, [x_nourriture, y_nourriture, taille_bloc, taille_bloc])\n",
    "            for bloc in serpent:\n",
    "                pygame.draw.rect(fenetre, blanc, [bloc[0], bloc[1], taille_bloc, taille_bloc])\n",
    "            afficher_score(score)\n",
    "            pygame.display.update()\n",
    "            horloge.tick(vitesse_serpent)\n",
    "\n",
    "        # Training and score tracking\n",
    "        agent.train_on_episode()\n",
    "        nb_episodes += 1\n",
    "        agent.games_played+=1\n",
    "        total_score += score\n",
    "        high_score = max(high_score, score)\n",
    "        avg_score = total_score / nb_episodes\n",
    "        scores.append(score)\n",
    "        avg_scores.append(avg_score)\n",
    "\n",
    "        # Update epsilon for exploration-exploitation balance\n",
    "        # agent.update_epsilon(nb_episodes)\n",
    "\n",
    "        print(f\"Episode: {nb_episodes}/{MAX_EPISODES}, Score: {score}, Avg Score: {avg_score:.2f}, High Score: {high_score}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "        # Sync target network and save model periodically\n",
    "        if nb_episodes % UPDATE_TARGET_EVERY == 0:\n",
    "            agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "        if nb_episodes % 100 == 0:\n",
    "            agent.model.save(f'snake_dqn_model_episode_{nb_episodes}.pth')\n",
    "    \n",
    "    pygame.quit()\n",
    "    agent.plot_history(scores, avg_scores)\n",
    "    print(f\"Training completed. Total episodes: {nb_episodes}, Final Avg Score: {avg_score:.2f}, High Score: {high_score}\")\n",
    "\n",
    "# Run the game\n",
    "jeu()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
