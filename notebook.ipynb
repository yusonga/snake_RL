{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake deep q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SONG\\AppData\\Local\\Temp\\ipykernel_16668\\470691393.py:175: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  current_state = torch.tensor(current_state, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 Score 0 Record: 0 Epsilon: 80\n",
      "Game 2 Score 0 Record: 0 Epsilon: 79\n",
      "Game 3 Score 0 Record: 0 Epsilon: 78\n",
      "Game 4 Score 0 Record: 0 Epsilon: 77\n",
      "Game 5 Score 0 Record: 0 Epsilon: 76\n",
      "Game 6 Score 0 Record: 0 Epsilon: 75\n",
      "Game 7 Score 0 Record: 0 Epsilon: 74\n",
      "Game 8 Score 0 Record: 0 Epsilon: 73\n",
      "Game 9 Score 0 Record: 0 Epsilon: 72\n",
      "Game 10 Score 0 Record: 0 Epsilon: 71\n",
      "Game 11 Score 0 Record: 0 Epsilon: 70\n",
      "Game 12 Score 0 Record: 0 Epsilon: 69\n",
      "Game 13 Score 0 Record: 0 Epsilon: 68\n",
      "Game 14 Score 0 Record: 0 Epsilon: 67\n",
      "Game 15 Score 0 Record: 0 Epsilon: 66\n",
      "Game 16 Score 0 Record: 0 Epsilon: 65\n",
      "Game 17 Score 0 Record: 0 Epsilon: 64\n",
      "Game 18 Score 0 Record: 0 Epsilon: 63\n",
      "Game 19 Score 1 Record: 1 Epsilon: 62\n",
      "Game 20 Score 1 Record: 1 Epsilon: 61\n",
      "Game 21 Score 0 Record: 1 Epsilon: 60\n",
      "Game 22 Score 0 Record: 1 Epsilon: 59\n",
      "Game 23 Score 0 Record: 1 Epsilon: 58\n",
      "Game 24 Score 0 Record: 1 Epsilon: 57\n",
      "Game 25 Score 0 Record: 1 Epsilon: 56\n",
      "Game 26 Score 0 Record: 1 Epsilon: 55\n",
      "Game 27 Score 0 Record: 1 Epsilon: 54\n",
      "Game 28 Score 1 Record: 1 Epsilon: 53\n",
      "Game 29 Score 1 Record: 1 Epsilon: 52\n",
      "Game 30 Score 0 Record: 1 Epsilon: 51\n",
      "Game 31 Score 1 Record: 1 Epsilon: 50\n",
      "Game 32 Score 1 Record: 1 Epsilon: 49\n",
      "Game 33 Score 0 Record: 1 Epsilon: 48\n",
      "Game 34 Score 0 Record: 1 Epsilon: 47\n",
      "Game 35 Score 0 Record: 1 Epsilon: 46\n",
      "Game 36 Score 1 Record: 1 Epsilon: 45\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 305\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGame\u001b[39m\u001b[38;5;124m'\u001b[39m, agent\u001b[38;5;241m.\u001b[39mgames_played, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m, score, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecord:\u001b[39m\u001b[38;5;124m'\u001b[39m, record, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpsilon:\u001b[39m\u001b[38;5;124m'\u001b[39m, agent\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 305\u001b[0m     \u001b[43mplay_snake_ai\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 288\u001b[0m, in \u001b[0;36mplay_snake_ai\u001b[1;34m()\u001b[0m\n\u001b[0;32m    286\u001b[0m state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mextract_state(game)\n\u001b[0;32m    287\u001b[0m move \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdecide_action(state)\n\u001b[1;32m--> 288\u001b[0m reward, done, score \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmove\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m new_state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mextract_state(game)\n\u001b[0;32m    290\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain_short_term(state, move, reward, new_state, done)\n",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m, in \u001b[0;36mSnakeAI.play_turn\u001b[1;34m(self, choice)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake_body\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Refresh display\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mticker\u001b[38;5;241m.\u001b[39mtick(GAME_SPEED)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward, game_end, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore\n",
      "Cell \u001b[1;32mIn[3], line 112\u001b[0m, in \u001b[0;36mSnakeAI.refresh_display\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrefresh_display\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOL_BLACK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake_body:\n\u001b[0;32m    115\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, COL_BLUE_PRIMARY, pygame\u001b[38;5;241m.\u001b[39mRect(part\u001b[38;5;241m.\u001b[39mx, part\u001b[38;5;241m.\u001b[39my, TILE_SIZE, TILE_SIZE))\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import pygame\n",
    "from enum import Enum\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "# Define font style\n",
    "font_style = pygame.font.SysFont(\"bahnschrift\", 25)\n",
    "\n",
    "class MoveDirection(Enum):\n",
    "    EAST = 1\n",
    "    WEST = 2\n",
    "    NORTH = 3\n",
    "    SOUTH = 4\n",
    "\n",
    "Coordinate = namedtuple('Coordinate', 'x, y')\n",
    "\n",
    "# RGB color definitions\n",
    "COL_WHITE = (255, 255, 255)\n",
    "COL_RED = (200, 0, 0)\n",
    "COL_BLUE_PRIMARY = (0, 0, 255)\n",
    "COL_BLUE_SECONDARY = (0, 100, 255)\n",
    "COL_BLACK = (0, 0, 0)\n",
    "\n",
    "TILE_SIZE = 20\n",
    "GAME_SPEED = 40\n",
    "\n",
    "class SnakeAI:\n",
    "\n",
    "    def __init__(self, width=640, height=480):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption('Snake AI')\n",
    "        self.ticker = pygame.time.Clock()\n",
    "        self.reset_game()\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.heading = MoveDirection.EAST\n",
    "        self.head = Coordinate(self.width / 2, self.height / 2)\n",
    "        self.snake_body = [self.head,\n",
    "                           Coordinate(self.head.x - TILE_SIZE, self.head.y),\n",
    "                           Coordinate(self.head.x - 2 * TILE_SIZE, self.head.y)]\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self.place_food()\n",
    "        self.iterations = 0\n",
    "\n",
    "    def place_food(self):\n",
    "        x = random.randint(0, (self.width - TILE_SIZE) // TILE_SIZE) * TILE_SIZE\n",
    "        y = random.randint(0, (self.height - TILE_SIZE) // TILE_SIZE) * TILE_SIZE\n",
    "        self.food = Coordinate(x, y)\n",
    "        if self.food in self.snake_body:\n",
    "            self.place_food()\n",
    "\n",
    "    def play_turn(self, choice):\n",
    "        self.iterations += 1\n",
    "\n",
    "        # Check for game quit event\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "\n",
    "        # Move snake\n",
    "        self.perform_move(choice)\n",
    "        self.snake_body.insert(0, self.head)\n",
    "\n",
    "        # Check for collision or timeout\n",
    "        reward = 0\n",
    "        game_end = False\n",
    "        if self.check_collision() or self.iterations > 100 * len(self.snake_body):\n",
    "            game_end = True\n",
    "            reward = -10\n",
    "            return reward, game_end, self.score\n",
    "\n",
    "        # Check for food consumption\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self.place_food()\n",
    "        else:\n",
    "            self.snake_body.pop()\n",
    "\n",
    "        # Refresh display\n",
    "        self.refresh_display()\n",
    "        self.ticker.tick(GAME_SPEED)\n",
    "\n",
    "        return reward, game_end, self.score\n",
    "\n",
    "    def check_collision(self, point=None):\n",
    "        if point is None:\n",
    "            point = self.head\n",
    "\n",
    "        # Boundary collision\n",
    "        if point.x > self.width - TILE_SIZE or point.x < 0 or point.y > self.height - TILE_SIZE or point.y < 0:\n",
    "            return True\n",
    "        # Self-collision\n",
    "        if point in self.snake_body[1:]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def refresh_display(self):\n",
    "        self.screen.fill(COL_BLACK)\n",
    "\n",
    "        for part in self.snake_body:\n",
    "            pygame.draw.rect(self.screen, COL_BLUE_PRIMARY, pygame.Rect(part.x, part.y, TILE_SIZE, TILE_SIZE))\n",
    "            pygame.draw.rect(self.screen, COL_BLUE_SECONDARY, pygame.Rect(part.x + 4, part.y + 4, 12, 12))\n",
    "\n",
    "        pygame.draw.rect(self.screen, COL_RED, pygame.Rect(self.food.x, self.food.y, TILE_SIZE, TILE_SIZE))\n",
    "\n",
    "        score_text = font_style.render(\"Score: \" + str(self.score), True, COL_WHITE)\n",
    "        self.screen.blit(score_text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def perform_move(self, choice):\n",
    "        directions = [MoveDirection.EAST, MoveDirection.SOUTH, MoveDirection.WEST, MoveDirection.NORTH]\n",
    "        idx = directions.index(self.heading)\n",
    "\n",
    "        if np.array_equal(choice, [1, 0, 0]):\n",
    "            new_heading = directions[idx]  # Keep same direction\n",
    "        elif np.array_equal(choice, [0, 1, 0]):\n",
    "            new_heading = directions[(idx + 1) % 4]  # Right turn\n",
    "        else:  # [0, 0, 1]\n",
    "            new_heading = directions[(idx - 1) % 4]  # Left turn\n",
    "\n",
    "        self.heading = new_heading\n",
    "        x, y = self.head.x, self.head.y\n",
    "        if self.heading == MoveDirection.EAST:\n",
    "            x += TILE_SIZE\n",
    "        elif self.heading == MoveDirection.WEST:\n",
    "            x -= TILE_SIZE\n",
    "        elif self.heading == MoveDirection.SOUTH:\n",
    "            y += TILE_SIZE\n",
    "        elif self.heading == MoveDirection.NORTH:\n",
    "            y -= TILE_SIZE\n",
    "\n",
    "        self.head = Coordinate(x, y)\n",
    "\n",
    "# Deep Q-Learning Model\n",
    "class DeepQNet(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(DeepQNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.layer2 = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, filename='model.pth'):\n",
    "        model_dir = './model'\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        torch.save(self.state_dict(), os.path.join(model_dir, filename))\n",
    "\n",
    "class QLearningTrainer:\n",
    "    def __init__(self, net_model, learning_rate, discount):\n",
    "        self.lr = learning_rate\n",
    "        self.discount = discount\n",
    "        self.model = net_model\n",
    "        self.optimizer = optim.Adam(net_model.parameters(), lr=self.lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def train_iteration(self, current_state, action_taken, reward_received, next_state, game_done):\n",
    "        current_state = torch.tensor(current_state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action_taken = torch.tensor(action_taken, dtype=torch.long)\n",
    "        reward_received = torch.tensor(reward_received, dtype=torch.float)\n",
    "\n",
    "        if len(current_state.shape) == 1:\n",
    "            current_state = torch.unsqueeze(current_state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action_taken = torch.unsqueeze(action_taken, 0)\n",
    "            reward_received = torch.unsqueeze(reward_received, 0)\n",
    "            game_done = (game_done,)\n",
    "\n",
    "        predictions = self.model(current_state)\n",
    "        target_values = predictions.clone()\n",
    "\n",
    "        for idx in range(len(game_done)):\n",
    "            Q_value = reward_received[idx]\n",
    "            if not game_done[idx]:\n",
    "                Q_value += self.discount * torch.max(self.model(next_state[idx]))\n",
    "            target_values[idx][torch.argmax(action_taken[idx]).item()] = Q_value\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(target_values, predictions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "MAX_MEM = 100_000\n",
    "BATCH = 1000\n",
    "LEARN_RATE = 0.001\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.games_played = 0\n",
    "        self.epsilon = 0  # randomness factor\n",
    "        self.gamma = 0.9  # discount factor\n",
    "        self.memory = deque(maxlen=MAX_MEM)\n",
    "        self.model = DeepQNet(11, 256, 3)\n",
    "        self.trainer = QLearningTrainer(self.model, learning_rate=LEARN_RATE, discount=self.gamma)\n",
    "\n",
    "    def extract_state(self, game):\n",
    "        head = game.snake_body[0]\n",
    "        left = Coordinate(head.x - TILE_SIZE, head.y)\n",
    "        right = Coordinate(head.x + TILE_SIZE, head.y)\n",
    "        up = Coordinate(head.x, head.y - TILE_SIZE)\n",
    "        down = Coordinate(head.x, head.y + TILE_SIZE)\n",
    "\n",
    "        dir_left = game.heading == MoveDirection.WEST\n",
    "        dir_right = game.heading == MoveDirection.EAST\n",
    "        dir_up = game.heading == MoveDirection.NORTH\n",
    "        dir_down = game.heading == MoveDirection.SOUTH\n",
    "\n",
    "        state = [\n",
    "            (dir_right and game.check_collision(right)) or\n",
    "            (dir_left and game.check_collision(left)) or\n",
    "            (dir_up and game.check_collision(up)) or\n",
    "            (dir_down and game.check_collision(down)),\n",
    "\n",
    "            (dir_up and game.check_collision(right)) or\n",
    "            (dir_down and game.check_collision(left)) or\n",
    "            (dir_left and game.check_collision(up)) or\n",
    "            (dir_right and game.check_collision(down)),\n",
    "\n",
    "            (dir_down and game.check_collision(right)) or\n",
    "            (dir_up and game.check_collision(left)) or\n",
    "            (dir_right and game.check_collision(up)) or\n",
    "            (dir_left and game.check_collision(down)),\n",
    "\n",
    "            dir_left, dir_right, dir_up, dir_down,\n",
    "\n",
    "            game.food.x < game.head.x,\n",
    "            game.food.x > game.head.x,\n",
    "            game.food.y < game.head.y,\n",
    "            game.food.y > game.head.y\n",
    "        ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, game_done):\n",
    "        self.memory.append((state, action, reward, next_state, game_done))\n",
    "\n",
    "    def train_long_term(self):\n",
    "        if len(self.memory) > BATCH:\n",
    "            sample = random.sample(self.memory, BATCH)\n",
    "        else:\n",
    "            sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*sample)\n",
    "        self.trainer.train_iteration(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def train_short_term(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_iteration(state, action, reward, next_state, done)\n",
    "\n",
    "    def decide_action(self, state):\n",
    "        self.epsilon = 80 - self.games_played\n",
    "        final_move = [0, 0, 0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state_tensor)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "\n",
    "def play_snake_ai():\n",
    "    record = 0\n",
    "    agent = DQNAgent()\n",
    "    game = SnakeAI()\n",
    "    while True:\n",
    "        state = agent.extract_state(game)\n",
    "        move = agent.decide_action(state)\n",
    "        reward, done, score = game.play_turn(move)\n",
    "        new_state = agent.extract_state(game)\n",
    "        agent.train_short_term(state, move, reward, new_state, done)\n",
    "        agent.store_experience(state, move, reward, new_state, done)\n",
    "\n",
    "        if done:\n",
    "            game.reset_game()\n",
    "            agent.games_played += 1\n",
    "            agent.train_long_term()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.games_played, 'Score', score, 'Record:', record, 'Epsilon:', agent.epsilon)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    play_snake_ai()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.15)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode: 1/1000, Score: 0, Avg Score: 0.00, High Score: 0, Epsilon: 80.0000\n",
      "Episode: 2/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 79.0000\n",
      "Episode: 3/1000, Score: 0, Avg Score: 0.33, High Score: 1, Epsilon: 78.0000\n",
      "Episode: 4/1000, Score: 0, Avg Score: 0.25, High Score: 1, Epsilon: 77.0000\n",
      "Episode: 5/1000, Score: 1, Avg Score: 0.40, High Score: 1, Epsilon: 76.0000\n",
      "Episode: 6/1000, Score: 0, Avg Score: 0.33, High Score: 1, Epsilon: 75.0000\n",
      "Episode: 7/1000, Score: 0, Avg Score: 0.29, High Score: 1, Epsilon: 74.0000\n",
      "Episode: 8/1000, Score: 0, Avg Score: 0.25, High Score: 1, Epsilon: 73.0000\n",
      "Episode: 9/1000, Score: 0, Avg Score: 0.22, High Score: 1, Epsilon: 72.0000\n",
      "Episode: 10/1000, Score: 1, Avg Score: 0.30, High Score: 1, Epsilon: 71.0000\n",
      "Episode: 11/1000, Score: 1, Avg Score: 0.36, High Score: 1, Epsilon: 70.0000\n",
      "Episode: 12/1000, Score: 0, Avg Score: 0.33, High Score: 1, Epsilon: 69.0000\n",
      "Episode: 13/1000, Score: 0, Avg Score: 0.31, High Score: 1, Epsilon: 68.0000\n",
      "Episode: 14/1000, Score: 1, Avg Score: 0.36, High Score: 1, Epsilon: 67.0000\n",
      "Episode: 15/1000, Score: 0, Avg Score: 0.33, High Score: 1, Epsilon: 66.0000\n",
      "Episode: 16/1000, Score: 1, Avg Score: 0.38, High Score: 1, Epsilon: 65.0000\n",
      "Episode: 17/1000, Score: 1, Avg Score: 0.41, High Score: 1, Epsilon: 64.0000\n",
      "Episode: 18/1000, Score: 0, Avg Score: 0.39, High Score: 1, Epsilon: 63.0000\n",
      "Episode: 19/1000, Score: 0, Avg Score: 0.37, High Score: 1, Epsilon: 62.0000\n",
      "Episode: 20/1000, Score: 1, Avg Score: 0.40, High Score: 1, Epsilon: 61.0000\n",
      "Episode: 21/1000, Score: 1, Avg Score: 0.43, High Score: 1, Epsilon: 60.0000\n",
      "Episode: 22/1000, Score: 0, Avg Score: 0.41, High Score: 1, Epsilon: 59.0000\n",
      "Episode: 23/1000, Score: 1, Avg Score: 0.43, High Score: 1, Epsilon: 58.0000\n",
      "Episode: 24/1000, Score: 1, Avg Score: 0.46, High Score: 1, Epsilon: 57.0000\n",
      "Episode: 25/1000, Score: 1, Avg Score: 0.48, High Score: 1, Epsilon: 56.0000\n",
      "Episode: 26/1000, Score: 0, Avg Score: 0.46, High Score: 1, Epsilon: 55.0000\n",
      "Episode: 27/1000, Score: 0, Avg Score: 0.44, High Score: 1, Epsilon: 54.0000\n",
      "Episode: 28/1000, Score: 0, Avg Score: 0.43, High Score: 1, Epsilon: 53.0000\n",
      "Episode: 29/1000, Score: 1, Avg Score: 0.45, High Score: 1, Epsilon: 52.0000\n",
      "Episode: 30/1000, Score: 1, Avg Score: 0.47, High Score: 1, Epsilon: 51.0000\n",
      "Episode: 31/1000, Score: 1, Avg Score: 0.48, High Score: 1, Epsilon: 50.0000\n",
      "Episode: 32/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 49.0000\n",
      "Episode: 33/1000, Score: 0, Avg Score: 0.48, High Score: 1, Epsilon: 48.0000\n",
      "Episode: 34/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 47.0000\n",
      "Episode: 35/1000, Score: 0, Avg Score: 0.49, High Score: 1, Epsilon: 46.0000\n",
      "Episode: 36/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 45.0000\n",
      "Episode: 37/1000, Score: 0, Avg Score: 0.49, High Score: 1, Epsilon: 44.0000\n",
      "Episode: 38/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 43.0000\n",
      "Episode: 39/1000, Score: 1, Avg Score: 0.51, High Score: 1, Epsilon: 42.0000\n",
      "Episode: 40/1000, Score: 1, Avg Score: 0.53, High Score: 1, Epsilon: 41.0000\n",
      "Episode: 41/1000, Score: 1, Avg Score: 0.54, High Score: 1, Epsilon: 40.0000\n",
      "Episode: 42/1000, Score: 1, Avg Score: 0.55, High Score: 1, Epsilon: 39.0000\n",
      "Episode: 43/1000, Score: 1, Avg Score: 0.56, High Score: 1, Epsilon: 38.0000\n",
      "Episode: 44/1000, Score: 1, Avg Score: 0.57, High Score: 1, Epsilon: 37.0000\n",
      "Episode: 45/1000, Score: 1, Avg Score: 0.58, High Score: 1, Epsilon: 36.0000\n",
      "Episode: 46/1000, Score: 0, Avg Score: 0.57, High Score: 1, Epsilon: 35.0000\n",
      "Episode: 47/1000, Score: 1, Avg Score: 0.57, High Score: 1, Epsilon: 34.0000\n",
      "Episode: 48/1000, Score: 1, Avg Score: 0.58, High Score: 1, Epsilon: 33.0000\n",
      "Episode: 49/1000, Score: 1, Avg Score: 0.59, High Score: 1, Epsilon: 32.0000\n",
      "Episode: 50/1000, Score: 1, Avg Score: 0.60, High Score: 1, Epsilon: 31.0000\n",
      "Episode: 51/1000, Score: 1, Avg Score: 0.61, High Score: 1, Epsilon: 30.0000\n",
      "Episode: 52/1000, Score: 1, Avg Score: 0.62, High Score: 1, Epsilon: 29.0000\n",
      "Episode: 53/1000, Score: 1, Avg Score: 0.62, High Score: 1, Epsilon: 28.0000\n",
      "Episode: 54/1000, Score: 1, Avg Score: 0.63, High Score: 1, Epsilon: 27.0000\n",
      "Episode: 55/1000, Score: 1, Avg Score: 0.64, High Score: 1, Epsilon: 26.0000\n",
      "Episode: 56/1000, Score: 0, Avg Score: 0.62, High Score: 1, Epsilon: 25.0000\n",
      "Episode: 57/1000, Score: 1, Avg Score: 0.63, High Score: 1, Epsilon: 24.0000\n",
      "Episode: 58/1000, Score: 1, Avg Score: 0.64, High Score: 1, Epsilon: 23.0000\n",
      "Episode: 59/1000, Score: 1, Avg Score: 0.64, High Score: 1, Epsilon: 22.0000\n",
      "Episode: 60/1000, Score: 0, Avg Score: 0.63, High Score: 1, Epsilon: 21.0000\n",
      "Episode: 61/1000, Score: 2, Avg Score: 0.66, High Score: 2, Epsilon: 20.0000\n",
      "Episode: 62/1000, Score: 1, Avg Score: 0.66, High Score: 2, Epsilon: 19.0000\n",
      "Episode: 63/1000, Score: 1, Avg Score: 0.67, High Score: 2, Epsilon: 18.0000\n",
      "Episode: 64/1000, Score: 1, Avg Score: 0.67, High Score: 2, Epsilon: 17.0000\n",
      "Episode: 65/1000, Score: 1, Avg Score: 0.68, High Score: 2, Epsilon: 16.0000\n",
      "Episode: 66/1000, Score: 1, Avg Score: 0.68, High Score: 2, Epsilon: 15.0000\n",
      "Episode: 67/1000, Score: 1, Avg Score: 0.69, High Score: 2, Epsilon: 14.0000\n",
      "Episode: 68/1000, Score: 1, Avg Score: 0.69, High Score: 2, Epsilon: 13.0000\n",
      "Episode: 69/1000, Score: 0, Avg Score: 0.68, High Score: 2, Epsilon: 12.0000\n",
      "Episode: 70/1000, Score: 1, Avg Score: 0.69, High Score: 2, Epsilon: 11.0000\n",
      "Episode: 71/1000, Score: 0, Avg Score: 0.68, High Score: 2, Epsilon: 10.0000\n",
      "Episode: 72/1000, Score: 0, Avg Score: 0.67, High Score: 2, Epsilon: 9.0000\n",
      "Episode: 73/1000, Score: 3, Avg Score: 0.70, High Score: 3, Epsilon: 8.0000\n",
      "Episode: 74/1000, Score: 1, Avg Score: 0.70, High Score: 3, Epsilon: 7.0000\n",
      "Episode: 75/1000, Score: 1, Avg Score: 0.71, High Score: 3, Epsilon: 6.0000\n",
      "Episode: 76/1000, Score: 0, Avg Score: 0.70, High Score: 3, Epsilon: 5.0000\n",
      "Episode: 77/1000, Score: 1, Avg Score: 0.70, High Score: 3, Epsilon: 4.0000\n",
      "Episode: 78/1000, Score: 2, Avg Score: 0.72, High Score: 3, Epsilon: 3.0000\n",
      "Episode: 79/1000, Score: 1, Avg Score: 0.72, High Score: 3, Epsilon: 2.0000\n",
      "Episode: 80/1000, Score: 1, Avg Score: 0.72, High Score: 3, Epsilon: 1.0000\n",
      "Episode: 81/1000, Score: 1, Avg Score: 0.73, High Score: 3, Epsilon: 0.0000\n",
      "Episode: 82/1000, Score: 1, Avg Score: 0.73, High Score: 3, Epsilon: -1.0000\n",
      "Episode: 83/1000, Score: 1, Avg Score: 0.73, High Score: 3, Epsilon: -2.0000\n",
      "Episode: 84/1000, Score: 2, Avg Score: 0.75, High Score: 3, Epsilon: -3.0000\n",
      "Episode: 85/1000, Score: 1, Avg Score: 0.75, High Score: 3, Epsilon: -4.0000\n",
      "Episode: 86/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -5.0000\n",
      "Episode: 87/1000, Score: 0, Avg Score: 0.75, High Score: 3, Epsilon: -6.0000\n",
      "Episode: 88/1000, Score: 2, Avg Score: 0.76, High Score: 3, Epsilon: -7.0000\n",
      "Episode: 89/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -8.0000\n",
      "Episode: 90/1000, Score: 1, Avg Score: 0.77, High Score: 3, Epsilon: -9.0000\n",
      "Episode: 91/1000, Score: 0, Avg Score: 0.76, High Score: 3, Epsilon: -10.0000\n",
      "Episode: 92/1000, Score: 0, Avg Score: 0.75, High Score: 3, Epsilon: -11.0000\n",
      "Episode: 93/1000, Score: 1, Avg Score: 0.75, High Score: 3, Epsilon: -12.0000\n",
      "Episode: 94/1000, Score: 2, Avg Score: 0.77, High Score: 3, Epsilon: -13.0000\n",
      "Episode: 95/1000, Score: 0, Avg Score: 0.76, High Score: 3, Epsilon: -14.0000\n",
      "Episode: 96/1000, Score: 0, Avg Score: 0.75, High Score: 3, Epsilon: -15.0000\n",
      "Episode: 97/1000, Score: 0, Avg Score: 0.74, High Score: 3, Epsilon: -16.0000\n",
      "Episode: 98/1000, Score: 0, Avg Score: 0.73, High Score: 3, Epsilon: -17.0000\n",
      "Episode: 99/1000, Score: 1, Avg Score: 0.74, High Score: 3, Epsilon: -18.0000\n",
      "Episode: 100/1000, Score: 1, Avg Score: 0.74, High Score: 3, Epsilon: -19.0000\n",
      "Episode: 101/1000, Score: 1, Avg Score: 0.74, High Score: 3, Epsilon: -20.0000\n",
      "Episode: 102/1000, Score: 2, Avg Score: 0.75, High Score: 3, Epsilon: -21.0000\n",
      "Episode: 103/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -22.0000\n",
      "Episode: 104/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -23.0000\n",
      "Episode: 105/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -24.0000\n",
      "Episode: 106/1000, Score: 0, Avg Score: 0.75, High Score: 3, Epsilon: -25.0000\n",
      "Episode: 107/1000, Score: 2, Avg Score: 0.77, High Score: 3, Epsilon: -26.0000\n",
      "Episode: 108/1000, Score: 2, Avg Score: 0.78, High Score: 3, Epsilon: -27.0000\n",
      "Episode: 109/1000, Score: 1, Avg Score: 0.78, High Score: 3, Epsilon: -28.0000\n",
      "Episode: 110/1000, Score: 1, Avg Score: 0.78, High Score: 3, Epsilon: -29.0000\n",
      "Episode: 111/1000, Score: 1, Avg Score: 0.78, High Score: 3, Epsilon: -30.0000\n",
      "Episode: 112/1000, Score: 1, Avg Score: 0.79, High Score: 3, Epsilon: -31.0000\n",
      "Episode: 113/1000, Score: 2, Avg Score: 0.80, High Score: 3, Epsilon: -32.0000\n",
      "Episode: 114/1000, Score: 1, Avg Score: 0.80, High Score: 3, Epsilon: -33.0000\n",
      "Episode: 115/1000, Score: 1, Avg Score: 0.80, High Score: 3, Epsilon: -34.0000\n",
      "Episode: 116/1000, Score: 1, Avg Score: 0.80, High Score: 3, Epsilon: -35.0000\n",
      "Episode: 117/1000, Score: 1, Avg Score: 0.80, High Score: 3, Epsilon: -36.0000\n",
      "Episode: 118/1000, Score: 2, Avg Score: 0.81, High Score: 3, Epsilon: -37.0000\n",
      "Episode: 119/1000, Score: 3, Avg Score: 0.83, High Score: 3, Epsilon: -38.0000\n",
      "Episode: 120/1000, Score: 1, Avg Score: 0.83, High Score: 3, Epsilon: -39.0000\n",
      "Episode: 121/1000, Score: 2, Avg Score: 0.84, High Score: 3, Epsilon: -40.0000\n",
      "Episode: 122/1000, Score: 1, Avg Score: 0.84, High Score: 3, Epsilon: -41.0000\n",
      "Episode: 123/1000, Score: 1, Avg Score: 0.85, High Score: 3, Epsilon: -42.0000\n",
      "Episode: 124/1000, Score: 1, Avg Score: 0.85, High Score: 3, Epsilon: -43.0000\n",
      "Episode: 125/1000, Score: 1, Avg Score: 0.85, High Score: 3, Epsilon: -44.0000\n",
      "Episode: 126/1000, Score: 1, Avg Score: 0.85, High Score: 3, Epsilon: -45.0000\n",
      "Episode: 127/1000, Score: 3, Avg Score: 0.87, High Score: 3, Epsilon: -46.0000\n",
      "Episode: 128/1000, Score: 1, Avg Score: 0.87, High Score: 3, Epsilon: -47.0000\n",
      "Episode: 129/1000, Score: 1, Avg Score: 0.87, High Score: 3, Epsilon: -48.0000\n",
      "Episode: 130/1000, Score: 1, Avg Score: 0.87, High Score: 3, Epsilon: -49.0000\n",
      "Episode: 131/1000, Score: 4, Avg Score: 0.89, High Score: 4, Epsilon: -50.0000\n",
      "Episode: 132/1000, Score: 1, Avg Score: 0.89, High Score: 4, Epsilon: -51.0000\n",
      "Episode: 133/1000, Score: 1, Avg Score: 0.89, High Score: 4, Epsilon: -52.0000\n",
      "Episode: 134/1000, Score: 3, Avg Score: 0.91, High Score: 4, Epsilon: -53.0000\n",
      "Episode: 135/1000, Score: 2, Avg Score: 0.92, High Score: 4, Epsilon: -54.0000\n",
      "Episode: 136/1000, Score: 2, Avg Score: 0.93, High Score: 4, Epsilon: -55.0000\n",
      "Episode: 137/1000, Score: 1, Avg Score: 0.93, High Score: 4, Epsilon: -56.0000\n",
      "Episode: 138/1000, Score: 2, Avg Score: 0.93, High Score: 4, Epsilon: -57.0000\n",
      "Episode: 139/1000, Score: 1, Avg Score: 0.94, High Score: 4, Epsilon: -58.0000\n",
      "Episode: 140/1000, Score: 1, Avg Score: 0.94, High Score: 4, Epsilon: -59.0000\n",
      "Episode: 141/1000, Score: 1, Avg Score: 0.94, High Score: 4, Epsilon: -60.0000\n",
      "Episode: 142/1000, Score: 1, Avg Score: 0.94, High Score: 4, Epsilon: -61.0000\n",
      "Episode: 143/1000, Score: 2, Avg Score: 0.94, High Score: 4, Epsilon: -62.0000\n",
      "Episode: 144/1000, Score: 2, Avg Score: 0.95, High Score: 4, Epsilon: -63.0000\n",
      "Episode: 145/1000, Score: 2, Avg Score: 0.96, High Score: 4, Epsilon: -64.0000\n",
      "Episode: 146/1000, Score: 2, Avg Score: 0.97, High Score: 4, Epsilon: -65.0000\n",
      "Episode: 147/1000, Score: 1, Avg Score: 0.97, High Score: 4, Epsilon: -66.0000\n",
      "Episode: 148/1000, Score: 1, Avg Score: 0.97, High Score: 4, Epsilon: -67.0000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#### final one!!!! some adjustment but works quite well than before!\n",
    "\n",
    "import pygame\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "# Pygame setup\n",
    "pygame.init()\n",
    "# Colors and window dimensions\n",
    "blanc, jaune, noir, rouge, vert, bleu = (255, 255, 255), (255, 255, 102), (0, 0, 0), (213, 50, 80), (0, 255, 0), (50, 153, 213)\n",
    "largeur_ecran, hauteur_ecran, taille_bloc, vitesse_serpent = 600, 400, 20, 40\n",
    "fenetre = pygame.display.set_mode((largeur_ecran, hauteur_ecran))\n",
    "pygame.display.set_caption(\"Snake Game - DQN\")\n",
    "horloge = pygame.time.Clock()\n",
    "police = pygame.font.SysFont(\"bahnschrift\", 25)\n",
    "\n",
    "# Display score\n",
    "def afficher_score(score):\n",
    "    valeur = police.render(\"Score: \" + str(score), True, jaune)\n",
    "    fenetre.blit(valeur, [0, 0])\n",
    "\n",
    "# Neural Network for DQN with two selectable architectures\n",
    "\n",
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, complex_model=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define network architecture based on model complexity\n",
    "        if complex_model:\n",
    "            # Complex model with two hidden layers\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "            self.fc3 = nn.Linear(hidden_size * 2, output_size)\n",
    "            self.complex = True\n",
    "        else:\n",
    "            # Simple model with one hidden layer\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "            self.complex = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Pass through additional layer if complex model\n",
    "        if self.complex:\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)  # Output layer\n",
    "        else:\n",
    "            x = self.fc2(x)  # Output layer for simple model\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        # Save model weights to specified directory\n",
    "        model_folder_path = './models'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "        \n",
    "        file_path = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "# Q-learning Trainer Class\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # Convert inputs to tensors, adding batch dimension if needed\n",
    "        state = torch.FloatTensor(state).unsqueeze(0) if len(state.shape) == 1 else torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0) if len(next_state.shape) == 1 else torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action]) if isinstance(action, int) else torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor([reward]) if isinstance(reward, (int, float)) else torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor([done]) if isinstance(done, bool) else torch.FloatTensor(done)\n",
    "\n",
    "        # Predicted Q values for current state-action pairs\n",
    "        pred = self.model(state).gather(1, action.view(-1, 1))\n",
    "\n",
    "        # Calculate target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_state).max(1)[0]\n",
    "            target_q_values = reward + (1 - done) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(pred, target_q_values.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "# Paramètres pour le DQN (modifiés)\n",
    "HIDDEN_SIZE = 128  # Increased hidden size\n",
    "OUTPUT_SIZE = 4\n",
    "BATCH_SIZE = 64  # Increased batch size\n",
    "MEMORY_SIZE = 200000  # Increased memory size\n",
    "UPDATE_TARGET_EVERY = 4  # Update target network less frequently\n",
    "START_TRAINING_THRESHOLD = BATCH_SIZE * 10\n",
    "MAX_EPISODES = 1000\n",
    "# Agent with model, training, and actions\n",
    "class AIAgent:\n",
    "    def __init__(self, input_size, hidden_size, output_size, gamma=0.99, lr=0.001, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.9, complex_model=False, max_episodes=1000, batch_size=128):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        \n",
    "        # Model initialization\n",
    "        self.model = Linear_QNet(input_size, hidden_size, output_size)\n",
    "        self.target_model = Linear_QNet(input_size, hidden_size, output_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.trainer = QTrainer(self.model, lr=lr, gamma=gamma)\n",
    "        \n",
    "        # Tracking episodes and batch size for training\n",
    "        self.games_played = 0\n",
    "        self.episode_memory = []\n",
    "        self.max_episodes = max_episodes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head = snake[-1]\n",
    "        point_l = [head[0] - taille_bloc, head[1]]\n",
    "        point_r = [head[0] + taille_bloc, head[1]]\n",
    "        point_u = [head[0], head[1] - taille_bloc]\n",
    "        point_d = [head[0], head[1] + taille_bloc]\n",
    "        \n",
    "        dir_l = head[0] > snake[-2][0] if len(snake) > 1 else False\n",
    "        dir_r = head[0] < snake[-2][0] if len(snake) > 1 else False\n",
    "        dir_u = head[1] > snake[-2][1] if len(snake) > 1 else False\n",
    "        dir_d = head[1] < snake[-2][1] if len(snake) > 1 else False\n",
    "\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and self.is_collision(point_r, snake)) or \n",
    "            (dir_l and self.is_collision(point_l, snake)) or \n",
    "            (dir_u and self.is_collision(point_u, snake)) or \n",
    "            (dir_d and self.is_collision(point_d, snake)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and self.is_collision(point_r, snake)) or \n",
    "            (dir_d and self.is_collision(point_l, snake)) or \n",
    "            (dir_l and self.is_collision(point_u, snake)) or \n",
    "            (dir_r and self.is_collision(point_d, snake)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and self.is_collision(point_r, snake)) or \n",
    "            (dir_u and self.is_collision(point_l, snake)) or \n",
    "            (dir_r and self.is_collision(point_u, snake)) or \n",
    "            (dir_l and self.is_collision(point_d, snake)),\n",
    "            \n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "            \n",
    "            # Food location \n",
    "            food[0] < head[0],  # food left\n",
    "            food[0] > head[0],  # food right\n",
    "            food[1] < head[1],  # food up\n",
    "            food[1] > head[1]   # food down\n",
    "        ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def is_collision(self, point, snake):\n",
    "        if point[0] < 0 or point[0] >= largeur_ecran or point[1] < 0 or point[1] >= hauteur_ecran:\n",
    "            return True\n",
    "        if point in snake[:-1]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Train on batch\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = 80 - self.games_played\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            return random.randint(0, 2)\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state_tensor)\n",
    "            return torch.argmax(prediction).item()\n",
    "\n",
    "   \n",
    "    def store_episode(self, state, action, reward, next_state, done):\n",
    "        self.episode_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_on_episode(self):\n",
    "        for experience in self.episode_memory:\n",
    "            self.remember(*experience)\n",
    "        \n",
    "        # Train only if enough memory is stored\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            self.train_long_memory()\n",
    "        \n",
    "        # Clear episode memory after training\n",
    "        self.episode_memory.clear()\n",
    "\n",
    "# RGB color values\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (200,0,0)\n",
    "BLUE1 = (0, 0, 255)\n",
    "BLUE2 = (0, 100, 255)\n",
    "BLACK = (0,0,0)\n",
    "def jeu():\n",
    "    agent = AIAgent(11, HIDDEN_SIZE, OUTPUT_SIZE, complex_model=True)  \n",
    "    nb_episodes, total_score, high_score = 0, 0, 0\n",
    "    scores, avg_scores = [], []\n",
    "    \n",
    "    while nb_episodes < MAX_EPISODES:\n",
    "        game_over = False\n",
    "        score = 0\n",
    "        serpent = [[largeur_ecran / 2, hauteur_ecran / 2]]\n",
    "        x_nourriture, y_nourriture = round(random.randrange(0, largeur_ecran - taille_bloc) / taille_bloc) * taille_bloc, round(random.randrange(0, hauteur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "        steps_without_food = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # Get the state and decide on an action\n",
    "            state = agent.get_state(serpent, (x_nourriture, y_nourriture))\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Move snake based on action\n",
    "            x_change, y_change = [(0, -taille_bloc), (0, taille_bloc), (-taille_bloc, 0), (taille_bloc, 0)][action]\n",
    "            nouvelle_tete = [serpent[-1][0] + x_change, serpent[-1][1] + y_change]\n",
    "            serpent.append(nouvelle_tete)\n",
    "\n",
    "            # Check for game over\n",
    "            if (nouvelle_tete[0] < 0 or nouvelle_tete[0] >= largeur_ecran or\n",
    "                nouvelle_tete[1] < 0 or nouvelle_tete[1] >= hauteur_ecran or\n",
    "                nouvelle_tete in serpent[:-1]):\n",
    "                game_over = True\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = -0.01  # Small survival reward\n",
    "\n",
    "                # Check for food consumption\n",
    "                if nouvelle_tete[0] == x_nourriture and nouvelle_tete[1] == y_nourriture:\n",
    "                    reward = 50  # Eating food reward\n",
    "                    score += 1\n",
    "                    x_nourriture = round(random.randrange(0, largeur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "                    y_nourriture = round(random.randrange(0, hauteur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "                    steps_without_food = 0\n",
    "                else:\n",
    "                    serpent.pop(0)  # Snake moves without growing\n",
    "\n",
    "                    # Reward adjustment for distance to food\n",
    "                    if len(serpent) > 1:\n",
    "                        distance_before = np.linalg.norm(np.array(serpent[-2]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    else:\n",
    "                        distance_before = np.linalg.norm(np.array(serpent[-1]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    distance_after = np.linalg.norm(np.array(nouvelle_tete) - np.array([x_nourriture, y_nourriture]))\n",
    "                    reward += 10 if distance_after < distance_before else -0.5\n",
    "                    # distance_before = np.linalg.norm(np.array(serpent[-2]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    # distance_after = np.linalg.norm(np.array(nouvelle_tete) - np.array([x_nourriture, y_nourriture]))\n",
    "                    # reward += 10 if distance_after < distance_before else -0.5\n",
    "                    reward += 0.1  # Additional survival reward\n",
    "\n",
    "                    steps_without_food += 1\n",
    "                    if steps_without_food > 100:  # Penalize long survival without eating\n",
    "                        reward -= 1\n",
    "                        steps_without_food = 0\n",
    "\n",
    "            # Prepare for the next step\n",
    "            next_state = agent.get_state(serpent, (x_nourriture, y_nourriture))\n",
    "            agent.store_episode(state, action, reward, next_state, game_over)\n",
    "\n",
    "            # Update Pygame window\n",
    "            fenetre.fill(BLACK)\n",
    "            pygame.draw.rect(fenetre, RED, [x_nourriture, y_nourriture, taille_bloc, taille_bloc])\n",
    "            for bloc in serpent:\n",
    "                pygame.draw.rect(fenetre, WHITE, [bloc[0], bloc[1], taille_bloc, taille_bloc])\n",
    "            afficher_score(score)\n",
    "            pygame.display.update()\n",
    "            horloge.tick(vitesse_serpent)\n",
    "\n",
    "        # Training and score tracking\n",
    "        agent.train_on_episode()\n",
    "        nb_episodes += 1\n",
    "        agent.games_played+=1\n",
    "        total_score += score\n",
    "        high_score = max(high_score, score)\n",
    "        avg_score = total_score / nb_episodes\n",
    "        scores.append(score)\n",
    "        avg_scores.append(avg_score)\n",
    "\n",
    "        # Update epsilon for exploration-exploitation balance\n",
    "        # agent.update_epsilon(nb_episodes)\n",
    "\n",
    "        print(f\"Episode: {nb_episodes}/{MAX_EPISODES}, Score: {score}, Avg Score: {avg_score:.2f}, High Score: {high_score}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "        # Sync target network and save model periodically\n",
    "        if nb_episodes % UPDATE_TARGET_EVERY == 0:\n",
    "            agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "        if nb_episodes % 100 == 0:\n",
    "            agent.model.save(f'snake_dqn_model_episode_{nb_episodes}.pth')\n",
    "    \n",
    "    pygame.quit()\n",
    "    agent.plot_history(scores, avg_scores)\n",
    "    print(f\"Training completed. Total episodes: {nb_episodes}, Final Avg Score: {avg_score:.2f}, High Score: {high_score}\")\n",
    "\n",
    "# Run the game\n",
    "jeu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.15)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "pygame.init()\n",
    "# font = pygame.font.Font('arial.ttf', 25)\n",
    "#font = pygame.font.SysFont('arial', 25)\n",
    "police = pygame.font.SysFont(\"bahnschrift\", 25)\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "\n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "# rgb colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (200,0,0)\n",
    "BLUE1 = (0, 0, 255)\n",
    "BLUE2 = (0, 100, 255)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "SPEED = 40\n",
    "\n",
    "class SnakeGameAI:\n",
    "\n",
    "    def __init__(self, w=640, h=480):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        # init display\n",
    "        self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        pygame.display.set_caption('Snake')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # init game state\n",
    "        self.direction = Direction.RIGHT\n",
    "\n",
    "        self.head = Point(self.w/2, self.h/2)\n",
    "        self.snake = [self.head,\n",
    "                      Point(self.head.x-BLOCK_SIZE, self.head.y),\n",
    "                      Point(self.head.x-(2*BLOCK_SIZE), self.head.y)]\n",
    "\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        self.frame_iteration = 0\n",
    "\n",
    "\n",
    "    def _place_food(self):\n",
    "        x = random.randint(0, (self.w-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        y = random.randint(0, (self.h-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:\n",
    "            self._place_food()\n",
    "\n",
    "\n",
    "    def play_step(self, action):\n",
    "        self.frame_iteration += 1\n",
    "        # 1. collect user input\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "        \n",
    "        # 2. move\n",
    "        self._move(action) # update the head\n",
    "        self.snake.insert(0, self.head)\n",
    "        \n",
    "        # 3. check if game over\n",
    "        reward = 0\n",
    "        game_over = False\n",
    "        if self.is_collision() or self.frame_iteration > 100*len(self.snake):\n",
    "            game_over = True\n",
    "            reward = -10\n",
    "            return reward, game_over, self.score\n",
    "\n",
    "        # 4. place new food or just move\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "        \n",
    "        # 5. update ui and clock\n",
    "        self._update_ui()\n",
    "        self.clock.tick(SPEED)\n",
    "        # 6. return game over and score\n",
    "        return reward, game_over, self.score\n",
    "\n",
    "\n",
    "    def is_collision(self, pt=None):\n",
    "        if pt is None:\n",
    "            pt = self.head\n",
    "        # hits boundary\n",
    "        if pt.x > self.w - BLOCK_SIZE or pt.x < 0 or pt.y > self.h - BLOCK_SIZE or pt.y < 0:\n",
    "            return True\n",
    "        # hits itself\n",
    "        if pt in self.snake[1:]:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def _update_ui(self):\n",
    "        self.display.fill(BLACK)\n",
    "\n",
    "        for pt in self.snake:\n",
    "            pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "            pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x+4, pt.y+4, 12, 12))\n",
    "\n",
    "        pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "\n",
    "        text = police.render(\"Score: \" + str(self.score), True, WHITE)\n",
    "        \n",
    "        self.display.blit(text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "\n",
    "\n",
    "    def _move(self, action):\n",
    "        # [straight, right, left]\n",
    "\n",
    "        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
    "        idx = clock_wise.index(self.direction)\n",
    "\n",
    "        if np.array_equal(action, [1, 0, 0]):\n",
    "            new_dir = clock_wise[idx] # no change\n",
    "        elif np.array_equal(action, [0, 1, 0]):\n",
    "            next_idx = (idx + 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # right turn r -> d -> l -> u\n",
    "        else: # [0, 0, 1]\n",
    "            next_idx = (idx - 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # left turn r -> u -> l -> d\n",
    "\n",
    "        self.direction = new_dir\n",
    "\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        if self.direction == Direction.RIGHT:\n",
    "            x += BLOCK_SIZE\n",
    "        elif self.direction == Direction.LEFT:\n",
    "            x -= BLOCK_SIZE\n",
    "        elif self.direction == Direction.DOWN:\n",
    "            y += BLOCK_SIZE\n",
    "        elif self.direction == Direction.UP:\n",
    "            y -= BLOCK_SIZE\n",
    "\n",
    "        self.head = Point(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "\n",
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)\n",
    "\n",
    "\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        # (n, x)\n",
    "\n",
    "        if len(state.shape) == 1:\n",
    "            # (1, x)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )\n",
    "\n",
    "        # 1: predicted Q values with current state\n",
    "        pred = self.model(state)\n",
    "\n",
    "        target = pred.clone()\n",
    "        for idx in range(len(done)):\n",
    "            Q_new = reward[idx]\n",
    "            if not done[idx]:\n",
    "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
    "\n",
    "            target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "    \n",
    "        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
    "        # pred.clone()\n",
    "        # preds[argmax(action)] = Q_new\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SONG\\AppData\\Local\\Temp\\ipykernel_12064\\253291230.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  state = torch.tensor(state, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 Score 0 Record: 0 Epsilon: 80\n",
      "Game 2 Score 0 Record: 0 Epsilon: 79\n",
      "Game 3 Score 0 Record: 0 Epsilon: 78\n",
      "Game 4 Score 0 Record: 0 Epsilon: 77\n",
      "Game 5 Score 0 Record: 0 Epsilon: 76\n",
      "Game 6 Score 0 Record: 0 Epsilon: 75\n",
      "Game 7 Score 1 Record: 1 Epsilon: 74\n",
      "Game 8 Score 0 Record: 1 Epsilon: 73\n",
      "Game 9 Score 0 Record: 1 Epsilon: 72\n",
      "Game 10 Score 0 Record: 1 Epsilon: 71\n",
      "Game 11 Score 0 Record: 1 Epsilon: 70\n",
      "Game 12 Score 1 Record: 1 Epsilon: 69\n",
      "Game 13 Score 1 Record: 1 Epsilon: 68\n",
      "Game 14 Score 0 Record: 1 Epsilon: 67\n",
      "Game 15 Score 1 Record: 1 Epsilon: 66\n",
      "Game 16 Score 0 Record: 1 Epsilon: 65\n",
      "Game 17 Score 0 Record: 1 Epsilon: 64\n",
      "Game 18 Score 1 Record: 1 Epsilon: 63\n",
      "Game 19 Score 1 Record: 1 Epsilon: 62\n",
      "Game 20 Score 1 Record: 1 Epsilon: 61\n",
      "Game 21 Score 1 Record: 1 Epsilon: 60\n",
      "Game 22 Score 1 Record: 1 Epsilon: 59\n",
      "Game 23 Score 0 Record: 1 Epsilon: 58\n",
      "Game 24 Score 1 Record: 1 Epsilon: 57\n",
      "Game 25 Score 3 Record: 3 Epsilon: 56\n",
      "Game 26 Score 0 Record: 3 Epsilon: 55\n",
      "Game 27 Score 3 Record: 3 Epsilon: 54\n",
      "Game 28 Score 1 Record: 3 Epsilon: 53\n",
      "Game 29 Score 2 Record: 3 Epsilon: 52\n",
      "Game 30 Score 1 Record: 3 Epsilon: 51\n",
      "Game 31 Score 0 Record: 3 Epsilon: 50\n",
      "Game 32 Score 2 Record: 3 Epsilon: 49\n",
      "Game 33 Score 3 Record: 3 Epsilon: 48\n",
      "Game 34 Score 1 Record: 3 Epsilon: 47\n",
      "Game 35 Score 3 Record: 3 Epsilon: 46\n",
      "Game 36 Score 0 Record: 3 Epsilon: 45\n",
      "Game 37 Score 1 Record: 3 Epsilon: 44\n",
      "Game 38 Score 0 Record: 3 Epsilon: 43\n",
      "Game 39 Score 1 Record: 3 Epsilon: 42\n",
      "Game 40 Score 0 Record: 3 Epsilon: 41\n",
      "Game 41 Score 0 Record: 3 Epsilon: 40\n",
      "Game 42 Score 0 Record: 3 Epsilon: 39\n",
      "Game 43 Score 0 Record: 3 Epsilon: 38\n",
      "Game 44 Score 1 Record: 3 Epsilon: 37\n",
      "Game 45 Score 1 Record: 3 Epsilon: 36\n",
      "Game 46 Score 1 Record: 3 Epsilon: 35\n",
      "Game 47 Score 1 Record: 3 Epsilon: 34\n",
      "Game 48 Score 0 Record: 3 Epsilon: 33\n",
      "Game 49 Score 2 Record: 3 Epsilon: 32\n",
      "Game 50 Score 1 Record: 3 Epsilon: 31\n",
      "Game 51 Score 2 Record: 3 Epsilon: 30\n",
      "Game 52 Score 1 Record: 3 Epsilon: 29\n",
      "Game 53 Score 3 Record: 3 Epsilon: 28\n",
      "Game 54 Score 1 Record: 3 Epsilon: 27\n",
      "Game 55 Score 3 Record: 3 Epsilon: 26\n",
      "Game 56 Score 2 Record: 3 Epsilon: 25\n",
      "Game 57 Score 1 Record: 3 Epsilon: 24\n",
      "Game 58 Score 1 Record: 3 Epsilon: 23\n",
      "Game 59 Score 1 Record: 3 Epsilon: 22\n",
      "Game 60 Score 8 Record: 8 Epsilon: 21\n",
      "Game 61 Score 6 Record: 8 Epsilon: 20\n",
      "Game 62 Score 3 Record: 8 Epsilon: 19\n",
      "Game 63 Score 4 Record: 8 Epsilon: 18\n",
      "Game 64 Score 4 Record: 8 Epsilon: 17\n",
      "Game 65 Score 0 Record: 8 Epsilon: 16\n",
      "Game 66 Score 5 Record: 8 Epsilon: 15\n",
      "Game 67 Score 4 Record: 8 Epsilon: 14\n",
      "Game 68 Score 2 Record: 8 Epsilon: 13\n",
      "Game 69 Score 4 Record: 8 Epsilon: 12\n",
      "Game 70 Score 3 Record: 8 Epsilon: 11\n",
      "Game 71 Score 6 Record: 8 Epsilon: 10\n",
      "Game 72 Score 7 Record: 8 Epsilon: 9\n",
      "Game 73 Score 8 Record: 8 Epsilon: 8\n",
      "Game 74 Score 4 Record: 8 Epsilon: 7\n",
      "Game 75 Score 8 Record: 8 Epsilon: 6\n",
      "Game 76 Score 11 Record: 11 Epsilon: 5\n",
      "Game 77 Score 3 Record: 11 Epsilon: 4\n",
      "Game 78 Score 16 Record: 16 Epsilon: 3\n",
      "Game 79 Score 3 Record: 16 Epsilon: 2\n",
      "Game 80 Score 13 Record: 16 Epsilon: 1\n",
      "Game 81 Score 33 Record: 33 Epsilon: 0\n",
      "Game 82 Score 10 Record: 33 Epsilon: -1\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 140\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[38;5;66;03m# plot_scores.append(score)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;66;03m# total_score += score\u001b[39;00m\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;66;03m# mean_score = total_score / agent.n_games\u001b[39;00m\n\u001b[0;32m    135\u001b[0m             \u001b[38;5;66;03m# plot_mean_scores.append(mean_score)\u001b[39;00m\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;66;03m# plot(plot_scores, plot_mean_scores)\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 111\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m final_move \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state_old)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# perform move and get new state\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m reward, done, score \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_move\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m state_new \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_state(game)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# train short memory\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 94\u001b[0m, in \u001b[0;36mSnakeGameAI.play_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# 5. update ui and clock\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ui\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(SPEED)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 6. return game over and score\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 114\u001b[0m, in \u001b[0;36mSnakeGameAI._update_ui\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_ui\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBLACK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake:\n\u001b[0;32m    117\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay, BLUE1, pygame\u001b[38;5;241m.\u001b[39mRect(pt\u001b[38;5;241m.\u001b[39mx, pt\u001b[38;5;241m.\u001b[39my, BLOCK_SIZE, BLOCK_SIZE))\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import deque\n",
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0 # randomness\n",
    "        self.gamma = 0.9 # discount rate\n",
    "        self.memory = deque(maxlen=MAX_MEMORY) # popleft()\n",
    "        self.model = Linear_QNet(11, 256, 3)\n",
    "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "\n",
    "\n",
    "    def get_state(self, game):\n",
    "        head = game.snake[0]\n",
    "        point_l = Point(head.x - 20, head.y)\n",
    "        point_r = Point(head.x + 20, head.y)\n",
    "        point_u = Point(head.x, head.y - 20)\n",
    "        point_d = Point(head.x, head.y + 20)\n",
    "        \n",
    "        dir_l = game.direction == Direction.LEFT\n",
    "        dir_r = game.direction == Direction.RIGHT\n",
    "        dir_u = game.direction == Direction.UP\n",
    "        dir_d = game.direction == Direction.DOWN\n",
    "\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and game.is_collision(point_r)) or \n",
    "            (dir_l and game.is_collision(point_l)) or \n",
    "            (dir_u and game.is_collision(point_u)) or \n",
    "            (dir_d and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and game.is_collision(point_r)) or \n",
    "            (dir_d and game.is_collision(point_l)) or \n",
    "            (dir_l and game.is_collision(point_u)) or \n",
    "            (dir_r and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and game.is_collision(point_r)) or \n",
    "            (dir_u and game.is_collision(point_l)) or \n",
    "            (dir_r and game.is_collision(point_u)) or \n",
    "            (dir_l and game.is_collision(point_d)),\n",
    "            \n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "            \n",
    "            # Food location \n",
    "            game.food.x < game.head.x,  # food left\n",
    "            game.food.x > game.head.x,  # food right\n",
    "            game.food.y < game.head.y,  # food up\n",
    "            game.food.y > game.head.y  # food down\n",
    "            ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # random moves: tradeoff exploration / exploitation\n",
    "        self.epsilon = 80 - self.n_games\n",
    "        final_move = [0,0,0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "\n",
    "\n",
    "def train():\n",
    "    record = 0\n",
    "    agent = Agent()\n",
    "    game = SnakeGameAI()\n",
    "    while True:\n",
    "        # get old state\n",
    "        state_old = agent.get_state(game)\n",
    "\n",
    "        # get move\n",
    "        final_move = agent.get_action(state_old)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(final_move)\n",
    "        state_new = agent.get_state(game)\n",
    "\n",
    "        # train short memory\n",
    "        agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        # remember\n",
    "        agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        if done:\n",
    "            # train long memory, plot result\n",
    "            game.reset()\n",
    "            agent.n_games += 1\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.n_games, 'Score', score, 'Record:', record,'Epsilon:' ,agent.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pygame\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class SnakeEnv:\n",
    "    def __init__(self):\n",
    "        pygame.init()\n",
    "        self.largeur_ecran = 600\n",
    "        self.hauteur_ecran = 400\n",
    "        self.taille_bloc = 10\n",
    "        self.vitesse_serpent = 15\n",
    "        self.fenetre = pygame.display.set_mode((self.largeur_ecran, self.hauteur_ecran))\n",
    "        pygame.display.set_caption(\"Snake AI\")\n",
    "        self.horloge = pygame.time.Clock()\n",
    "\n",
    "        # Définir les actions possibles : [0: Gauche, 1: Droite, 2: Haut, 3: Bas]\n",
    "        self.actions = [0, 1, 2, 3]\n",
    "\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\" Réinitialiser l'état du jeu \"\"\"\n",
    "        self.x = self.largeur_ecran / 2\n",
    "        self.y = self.hauteur_ecran / 2\n",
    "        self.x_changement = 0\n",
    "        self.y_changement = 0\n",
    "        self.serpent = [[self.x, self.y]]\n",
    "        self.longueur_serpent = 1\n",
    "        self.nourriture = self.generer_nourriture()\n",
    "        self.score = 0\n",
    "        return self.get_state()\n",
    "\n",
    "    def generer_nourriture(self):\n",
    "        \"\"\" Générer une nouvelle position de nourriture \"\"\"\n",
    "        return [round(random.randrange(0, self.largeur_ecran - self.taille_bloc) / 10.0) * 10.0,\n",
    "                round(random.randrange(0, self.hauteur_ecran - self.taille_bloc) / 10.0) * 10.0]\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\" Retourner un vecteur d'état représentant la situation actuelle \"\"\"\n",
    "        return np.array([self.x, self.y, self.nourriture[0], self.nourriture[1]])\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\" Faire avancer le jeu d'un pas en fonction de l'action choisie \"\"\"\n",
    "        # Changer la direction en fonction de l'action\n",
    "        if action == 0:  # Gauche\n",
    "            self.x_changement = -self.taille_bloc\n",
    "            self.y_changement = 0\n",
    "        elif action == 1:  # Droite\n",
    "            self.x_changement = self.taille_bloc\n",
    "            self.y_changement = 0\n",
    "        elif action == 2:  # Haut\n",
    "            self.y_changement = -self.taille_bloc\n",
    "            self.x_changement = 0\n",
    "        elif action == 3:  # Bas\n",
    "            self.y_changement = self.taille_bloc\n",
    "            self.x_changement = 0\n",
    "\n",
    "        # Mettre à jour la position du serpent\n",
    "        self.x += self.x_changement\n",
    "        self.y += self.y_changement\n",
    "        self.serpent.append([self.x, self.y])\n",
    "\n",
    "        if len(self.serpent) > self.longueur_serpent:\n",
    "            del self.serpent[0]\n",
    "\n",
    "        # Vérifier la collision avec les bords ou avec soi-même\n",
    "        done = False\n",
    "        if self.x >= self.largeur_ecran or self.x < 0 or self.y >= self.hauteur_ecran or self.y < 0:\n",
    "            done = True\n",
    "        for bloc in self.serpent[:-1]:\n",
    "            if bloc == [self.x, self.y]:\n",
    "                done = True\n",
    "\n",
    "        # Gestion de la nourriture\n",
    "        reward = 0\n",
    "        if self.x == self.nourriture[0] and self.y == self.nourriture[1]:\n",
    "            self.nourriture = self.generer_nourriture()\n",
    "            self.longueur_serpent += 1\n",
    "            reward = 10  # Récompense pour avoir mangé la nourriture\n",
    "            self.score += 1\n",
    "\n",
    "        # Faible pénalité pour chaque mouvement \n",
    "        reward -= 0.1\n",
    "\n",
    "        return self.get_state(), reward, done\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\" Dessiner l'état actuel du jeu \"\"\"\n",
    "        self.fenetre.fill((0, 0, 0))  # Fond noir\n",
    "        # Dessiner la nourriture\n",
    "        pygame.draw.rect(self.fenetre, (0, 255, 0), [self.nourriture[0], self.nourriture[1], self.taille_bloc, self.taille_bloc])\n",
    "        # Dessiner le serpent\n",
    "        for bloc in self.serpent:\n",
    "            pygame.draw.rect(self.fenetre, (255, 255, 255), [bloc[0], bloc[1], self.taille_bloc, self.taille_bloc])\n",
    "\n",
    "        pygame.display.update()\n",
    "        self.horloge.tick(self.vitesse_serpent)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DQN:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        # Hyperparamètres de DQN\n",
    "        self.gamma = 0.95  # Facteur de discount\n",
    "        self.epsilon = 1.0  # Facteur d'exploration\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.learning_rate = 0.001\n",
    "        self.batch_size = 64\n",
    "\n",
    "        # Construction du modèle après initialisation des hyperparamètres\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Construit le modèle de réseau neuronal pour l'agent DQN\"\"\"\n",
    "        model = Sequential()\n",
    "        model.add(Dense(24, input_shape=(4,), activation='relu'))  # Correction de input_shape\n",
    "        model.add(Dense(24, activation='relu'))\n",
    "        model.add(Dense(4, activation='linear'))  # 4 sorties pour chaque action possible\n",
    "        \n",
    "        # Assure que la variable learning_rate est bien utilisée ici\n",
    "        model.compile(loss='mse', optimizer=tf.keras.optimizers.Adam(learning_rate=self.learning_rate))  # Correction\n",
    "        return model\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Enregistre une expérience dans la mémoire\"\"\"\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Choisit une action selon epsilon-greedy\"\"\"\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.choice(self.env.actions)  # Exploration\n",
    "        q_values = self.model.predict(state)\n",
    "        return np.argmax(q_values[0])  # Exploitation\n",
    "\n",
    "    def replay(self):\n",
    "        \"\"\"Entraîne le modèle sur un échantillon de la mémoire\"\"\"\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sélectionne un minibatch aléatoire\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mezio\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1/1000 - Score : 85\n",
      "Episode 4/1000 - Score : 216\n",
      "Episode 5/1000 - Score : 355\n",
      "Episode 6/1000 - Score : 234\n",
      "Episode 10/1000 - Score : 200\n",
      "Episode 11/1000 - Score : 469\n",
      "Episode 16/1000 - Score : 447\n",
      "Episode 20/1000 - Score : 332\n",
      "Episode 22/1000 - Score : 128\n",
      "Episode 24/1000 - Score : 474\n",
      "Episode 26/1000 - Score : 496\n",
      "Episode 28/1000 - Score : 304\n",
      "Episode 30/1000 - Score : 138\n",
      "Episode 31/1000 - Score : 421\n",
      "Episode 32/1000 - Score : 484\n",
      "Episode 34/1000 - Score : 133\n",
      "Episode 38/1000 - Score : 368\n",
      "Episode 40/1000 - Score : 227\n",
      "Episode 41/1000 - Score : 329\n",
      "Episode 42/1000 - Score : 350\n",
      "Episode 44/1000 - Score : 394\n",
      "Episode 47/1000 - Score : 177\n",
      "Episode 58/1000 - Score : 142\n",
      "Episode 60/1000 - Score : 368\n",
      "Episode 61/1000 - Score : 332\n",
      "Episode 67/1000 - Score : 479\n",
      "Episode 68/1000 - Score : 339\n",
      "Episode 70/1000 - Score : 413\n",
      "Episode 71/1000 - Score : 396\n",
      "Episode 74/1000 - Score : 402\n",
      "Episode 77/1000 - Score : 316\n",
      "Episode 79/1000 - Score : 446\n",
      "Episode 80/1000 - Score : 473\n",
      "Episode 85/1000 - Score : 393\n",
      "Episode 86/1000 - Score : 471\n",
      "Episode 87/1000 - Score : 263\n",
      "Episode 89/1000 - Score : 258\n",
      "Episode 90/1000 - Score : 465\n",
      "Episode 93/1000 - Score : 213\n",
      "Episode 99/1000 - Score : 115\n",
      "Episode 102/1000 - Score : 497\n",
      "Episode 103/1000 - Score : 303\n",
      "Episode 104/1000 - Score : 242\n",
      "Episode 105/1000 - Score : 451\n",
      "Episode 108/1000 - Score : 444\n",
      "Episode 110/1000 - Score : 348\n",
      "Episode 111/1000 - Score : 254\n",
      "Episode 112/1000 - Score : 189\n",
      "Episode 113/1000 - Score : 115\n",
      "Episode 114/1000 - Score : 415\n",
      "Episode 115/1000 - Score : 407\n",
      "Episode 117/1000 - Score : 451\n",
      "Episode 120/1000 - Score : 416\n",
      "Episode 121/1000 - Score : 183\n",
      "Episode 123/1000 - Score : 213\n",
      "Episode 124/1000 - Score : 349\n",
      "Episode 125/1000 - Score : 354\n",
      "Episode 127/1000 - Score : 307\n",
      "Episode 128/1000 - Score : 380\n",
      "Episode 131/1000 - Score : 428\n",
      "Episode 132/1000 - Score : 438\n",
      "Episode 133/1000 - Score : 201\n",
      "Episode 136/1000 - Score : 358\n",
      "Episode 139/1000 - Score : 439\n",
      "Episode 141/1000 - Score : 329\n",
      "Episode 146/1000 - Score : 431\n",
      "Episode 147/1000 - Score : 243\n",
      "Episode 148/1000 - Score : 200\n",
      "Episode 152/1000 - Score : 303\n",
      "Episode 155/1000 - Score : 386\n",
      "Episode 160/1000 - Score : 63\n",
      "Episode 161/1000 - Score : 489\n",
      "Episode 163/1000 - Score : 391\n",
      "Episode 167/1000 - Score : 439\n",
      "Episode 169/1000 - Score : 385\n",
      "Episode 170/1000 - Score : 304\n",
      "Episode 171/1000 - Score : 416\n",
      "Episode 174/1000 - Score : 358\n",
      "Episode 178/1000 - Score : 367\n",
      "Episode 179/1000 - Score : 163\n",
      "Episode 184/1000 - Score : 306\n",
      "Episode 185/1000 - Score : 355\n",
      "Episode 186/1000 - Score : 215\n",
      "Episode 191/1000 - Score : 189\n",
      "Episode 192/1000 - Score : 410\n",
      "Episode 195/1000 - Score : 295\n",
      "Episode 197/1000 - Score : 181\n",
      "Episode 198/1000 - Score : 412\n",
      "Episode 201/1000 - Score : 371\n",
      "Episode 202/1000 - Score : 388\n",
      "Episode 203/1000 - Score : 427\n",
      "Episode 204/1000 - Score : 314\n",
      "Episode 206/1000 - Score : 258\n",
      "Episode 208/1000 - Score : 263\n",
      "Episode 209/1000 - Score : 477\n",
      "Episode 210/1000 - Score : 372\n",
      "Episode 212/1000 - Score : 99\n",
      "Episode 215/1000 - Score : 260\n",
      "Episode 220/1000 - Score : 466\n",
      "Episode 222/1000 - Score : 463\n",
      "Episode 225/1000 - Score : 406\n",
      "Episode 227/1000 - Score : 377\n",
      "Episode 231/1000 - Score : 160\n",
      "Episode 232/1000 - Score : 172\n",
      "Episode 233/1000 - Score : 257\n",
      "Episode 234/1000 - Score : 319\n",
      "Episode 236/1000 - Score : 219\n",
      "Episode 237/1000 - Score : 487\n",
      "Episode 241/1000 - Score : 170\n",
      "Episode 244/1000 - Score : 265\n",
      "Episode 245/1000 - Score : 281\n",
      "Episode 246/1000 - Score : 441\n",
      "Episode 249/1000 - Score : 208\n",
      "Episode 250/1000 - Score : 339\n",
      "Episode 254/1000 - Score : 496\n",
      "Episode 255/1000 - Score : 380\n",
      "Episode 257/1000 - Score : 318\n",
      "Episode 258/1000 - Score : 263\n",
      "Episode 259/1000 - Score : 312\n",
      "Episode 261/1000 - Score : 273\n",
      "Episode 262/1000 - Score : 285\n",
      "Episode 264/1000 - Score : 324\n",
      "Episode 265/1000 - Score : 319\n",
      "Episode 266/1000 - Score : 282\n",
      "Episode 267/1000 - Score : 205\n",
      "Episode 268/1000 - Score : 468\n",
      "Episode 270/1000 - Score : 476\n",
      "Episode 271/1000 - Score : 205\n",
      "Episode 275/1000 - Score : 469\n",
      "Episode 278/1000 - Score : 130\n",
      "Episode 279/1000 - Score : 412\n",
      "Episode 283/1000 - Score : 256\n",
      "Episode 286/1000 - Score : 375\n",
      "Episode 287/1000 - Score : 410\n",
      "Episode 293/1000 - Score : 415\n",
      "Episode 295/1000 - Score : 437\n",
      "Episode 297/1000 - Score : 441\n",
      "Episode 298/1000 - Score : 438\n",
      "Episode 302/1000 - Score : 253\n",
      "Episode 303/1000 - Score : 226\n",
      "Episode 308/1000 - Score : 234\n",
      "Episode 309/1000 - Score : 394\n",
      "Episode 310/1000 - Score : 381\n",
      "Episode 311/1000 - Score : 267\n",
      "Episode 312/1000 - Score : 152\n",
      "Episode 314/1000 - Score : 209\n",
      "Episode 316/1000 - Score : 241\n",
      "Episode 317/1000 - Score : 273\n",
      "Episode 320/1000 - Score : 356\n",
      "Episode 321/1000 - Score : 403\n",
      "Episode 322/1000 - Score : 322\n",
      "Episode 325/1000 - Score : 488\n",
      "Episode 328/1000 - Score : 424\n",
      "Episode 333/1000 - Score : 114\n",
      "Episode 334/1000 - Score : 229\n",
      "Episode 338/1000 - Score : 295\n",
      "Episode 339/1000 - Score : 232\n",
      "Episode 341/1000 - Score : 492\n",
      "Episode 342/1000 - Score : 282\n",
      "Episode 343/1000 - Score : 277\n",
      "Episode 344/1000 - Score : 208\n",
      "Episode 347/1000 - Score : 305\n",
      "Episode 350/1000 - Score : 469\n",
      "Episode 353/1000 - Score : 130\n",
      "Episode 354/1000 - Score : 214\n",
      "Episode 355/1000 - Score : 133\n",
      "Episode 356/1000 - Score : 134\n",
      "Episode 362/1000 - Score : 294\n",
      "Episode 368/1000 - Score : 456\n",
      "Episode 369/1000 - Score : 234\n",
      "Episode 372/1000 - Score : 181\n",
      "Episode 375/1000 - Score : 227\n",
      "Episode 379/1000 - Score : 303\n",
      "Episode 380/1000 - Score : 181\n",
      "Episode 381/1000 - Score : 182\n",
      "Episode 382/1000 - Score : 411\n",
      "Episode 384/1000 - Score : 391\n",
      "Episode 385/1000 - Score : 419\n",
      "Episode 386/1000 - Score : 411\n",
      "Episode 387/1000 - Score : 199\n",
      "Episode 391/1000 - Score : 122\n",
      "Episode 393/1000 - Score : 314\n",
      "Episode 394/1000 - Score : 444\n",
      "Episode 397/1000 - Score : 339\n",
      "Episode 398/1000 - Score : 292\n",
      "Episode 400/1000 - Score : 250\n",
      "Episode 401/1000 - Score : 154\n",
      "Episode 402/1000 - Score : 405\n",
      "Episode 406/1000 - Score : 421\n",
      "Episode 407/1000 - Score : 277\n",
      "Episode 410/1000 - Score : 240\n",
      "Episode 415/1000 - Score : 186\n",
      "Episode 417/1000 - Score : 134\n",
      "Episode 418/1000 - Score : 328\n",
      "Episode 419/1000 - Score : 433\n",
      "Episode 421/1000 - Score : 493\n",
      "Episode 424/1000 - Score : 425\n",
      "Episode 426/1000 - Score : 281\n",
      "Episode 427/1000 - Score : 422\n",
      "Episode 428/1000 - Score : 145\n",
      "Episode 429/1000 - Score : 453\n",
      "Episode 430/1000 - Score : 465\n",
      "Episode 431/1000 - Score : 408\n",
      "Episode 433/1000 - Score : 293\n",
      "Episode 436/1000 - Score : 403\n",
      "Episode 437/1000 - Score : 415\n",
      "Episode 439/1000 - Score : 331\n",
      "Episode 440/1000 - Score : 428\n",
      "Episode 442/1000 - Score : 439\n",
      "Episode 443/1000 - Score : 192\n",
      "Episode 448/1000 - Score : 432\n",
      "Episode 449/1000 - Score : 402\n",
      "Episode 450/1000 - Score : 225\n",
      "Episode 451/1000 - Score : 379\n",
      "Episode 454/1000 - Score : 102\n",
      "Episode 455/1000 - Score : 255\n",
      "Episode 456/1000 - Score : 494\n",
      "Episode 458/1000 - Score : 111\n",
      "Episode 459/1000 - Score : 476\n",
      "Episode 460/1000 - Score : 191\n",
      "Episode 465/1000 - Score : 244\n",
      "Episode 466/1000 - Score : 461\n",
      "Episode 467/1000 - Score : 294\n",
      "Episode 468/1000 - Score : 277\n",
      "Episode 469/1000 - Score : 451\n",
      "Episode 473/1000 - Score : 206\n",
      "Episode 474/1000 - Score : 396\n",
      "Episode 477/1000 - Score : 141\n",
      "Episode 478/1000 - Score : 143\n",
      "Episode 480/1000 - Score : 267\n",
      "Episode 483/1000 - Score : 470\n",
      "Episode 484/1000 - Score : 185\n",
      "Episode 485/1000 - Score : 320\n",
      "Episode 486/1000 - Score : 292\n",
      "Episode 489/1000 - Score : 412\n",
      "Episode 490/1000 - Score : 216\n",
      "Episode 492/1000 - Score : 425\n",
      "Episode 493/1000 - Score : 194\n",
      "Episode 495/1000 - Score : 447\n",
      "Episode 497/1000 - Score : 128\n",
      "Episode 498/1000 - Score : 310\n",
      "Episode 502/1000 - Score : 200\n",
      "Episode 506/1000 - Score : 345\n",
      "Episode 507/1000 - Score : 244\n",
      "Episode 510/1000 - Score : 466\n",
      "Episode 511/1000 - Score : 233\n",
      "Episode 514/1000 - Score : 269\n",
      "Episode 518/1000 - Score : 455\n",
      "Episode 519/1000 - Score : 211\n",
      "Episode 522/1000 - Score : 218\n",
      "Episode 523/1000 - Score : 437\n",
      "Episode 524/1000 - Score : 433\n",
      "Episode 526/1000 - Score : 452\n",
      "Episode 529/1000 - Score : 367\n",
      "Episode 532/1000 - Score : 237\n",
      "Episode 537/1000 - Score : 423\n",
      "Episode 541/1000 - Score : 328\n",
      "Episode 542/1000 - Score : 175\n",
      "Episode 544/1000 - Score : 464\n",
      "Episode 545/1000 - Score : 153\n",
      "Episode 547/1000 - Score : 374\n",
      "Episode 548/1000 - Score : 195\n",
      "Episode 550/1000 - Score : 303\n",
      "Episode 551/1000 - Score : 347\n",
      "Episode 552/1000 - Score : 144\n",
      "Episode 553/1000 - Score : 299\n",
      "Episode 554/1000 - Score : 464\n",
      "Episode 555/1000 - Score : 290\n",
      "Episode 556/1000 - Score : 352\n",
      "Episode 558/1000 - Score : 367\n",
      "Episode 561/1000 - Score : 188\n",
      "Episode 562/1000 - Score : 450\n",
      "Episode 564/1000 - Score : 493\n",
      "Episode 570/1000 - Score : 225\n",
      "Episode 571/1000 - Score : 255\n",
      "Episode 573/1000 - Score : 302\n",
      "Episode 575/1000 - Score : 358\n",
      "Episode 576/1000 - Score : 258\n",
      "Episode 582/1000 - Score : 84\n",
      "Episode 583/1000 - Score : 254\n",
      "Episode 586/1000 - Score : 209\n",
      "Episode 589/1000 - Score : 281\n",
      "Episode 593/1000 - Score : 397\n",
      "Episode 597/1000 - Score : 196\n",
      "Episode 598/1000 - Score : 278\n",
      "Episode 599/1000 - Score : 256\n",
      "Episode 602/1000 - Score : 155\n",
      "Episode 603/1000 - Score : 401\n",
      "Episode 607/1000 - Score : 388\n",
      "Episode 610/1000 - Score : 395\n",
      "Episode 613/1000 - Score : 179\n",
      "Episode 614/1000 - Score : 181\n",
      "Episode 615/1000 - Score : 309\n",
      "Episode 617/1000 - Score : 363\n",
      "Episode 619/1000 - Score : 347\n",
      "Episode 621/1000 - Score : 417\n",
      "Episode 623/1000 - Score : 364\n",
      "Episode 626/1000 - Score : 372\n",
      "Episode 630/1000 - Score : 437\n",
      "Episode 631/1000 - Score : 300\n",
      "Episode 632/1000 - Score : 290\n",
      "Episode 633/1000 - Score : 368\n",
      "Episode 635/1000 - Score : 207\n",
      "Episode 641/1000 - Score : 306\n",
      "Episode 644/1000 - Score : 288\n",
      "Episode 648/1000 - Score : 376\n",
      "Episode 649/1000 - Score : 196\n",
      "Episode 653/1000 - Score : 149\n",
      "Episode 655/1000 - Score : 393\n",
      "Episode 656/1000 - Score : 219\n",
      "Episode 657/1000 - Score : 425\n",
      "Episode 658/1000 - Score : 344\n",
      "Episode 661/1000 - Score : 226\n",
      "Episode 662/1000 - Score : 390\n",
      "Episode 665/1000 - Score : 199\n",
      "Episode 667/1000 - Score : 405\n",
      "Episode 668/1000 - Score : 288\n",
      "Episode 673/1000 - Score : 345\n",
      "Episode 675/1000 - Score : 418\n",
      "Episode 677/1000 - Score : 135\n",
      "Episode 679/1000 - Score : 318\n",
      "Episode 683/1000 - Score : 218\n",
      "Episode 684/1000 - Score : 354\n",
      "Episode 687/1000 - Score : 406\n",
      "Episode 688/1000 - Score : 309\n",
      "Episode 689/1000 - Score : 204\n",
      "Episode 693/1000 - Score : 486\n",
      "Episode 694/1000 - Score : 343\n",
      "Episode 695/1000 - Score : 431\n",
      "Episode 701/1000 - Score : 457\n",
      "Episode 702/1000 - Score : 201\n",
      "Episode 704/1000 - Score : 239\n",
      "Episode 707/1000 - Score : 494\n",
      "Episode 708/1000 - Score : 405\n",
      "Episode 709/1000 - Score : 473\n",
      "Episode 710/1000 - Score : 359\n",
      "Episode 711/1000 - Score : 279\n",
      "Episode 713/1000 - Score : 476\n",
      "Episode 716/1000 - Score : 286\n",
      "Episode 717/1000 - Score : 199\n",
      "Episode 724/1000 - Score : 320\n",
      "Episode 727/1000 - Score : 467\n",
      "Episode 729/1000 - Score : 269\n",
      "Episode 730/1000 - Score : 308\n",
      "Episode 732/1000 - Score : 221\n",
      "Episode 733/1000 - Score : 434\n",
      "Episode 735/1000 - Score : 359\n",
      "Episode 736/1000 - Score : 331\n",
      "Episode 737/1000 - Score : 177\n",
      "Episode 738/1000 - Score : 282\n",
      "Episode 739/1000 - Score : 394\n",
      "Episode 740/1000 - Score : 258\n",
      "Episode 741/1000 - Score : 291\n",
      "Episode 744/1000 - Score : 268\n",
      "Episode 746/1000 - Score : 288\n",
      "Episode 749/1000 - Score : 222\n",
      "Episode 754/1000 - Score : 396\n",
      "Episode 755/1000 - Score : 246\n",
      "Episode 758/1000 - Score : 379\n",
      "Episode 759/1000 - Score : 289\n",
      "Episode 761/1000 - Score : 273\n",
      "Episode 764/1000 - Score : 189\n",
      "Episode 765/1000 - Score : 206\n",
      "Episode 766/1000 - Score : 282\n",
      "Episode 769/1000 - Score : 365\n",
      "Episode 770/1000 - Score : 406\n",
      "Episode 772/1000 - Score : 310\n",
      "Episode 775/1000 - Score : 315\n",
      "Episode 776/1000 - Score : 133\n",
      "Episode 777/1000 - Score : 361\n",
      "Episode 778/1000 - Score : 329\n",
      "Episode 779/1000 - Score : 463\n",
      "Episode 781/1000 - Score : 141\n",
      "Episode 784/1000 - Score : 491\n",
      "Episode 785/1000 - Score : 97\n",
      "Episode 791/1000 - Score : 252\n",
      "Episode 792/1000 - Score : 278\n",
      "Episode 796/1000 - Score : 294\n",
      "Episode 798/1000 - Score : 196\n",
      "Episode 802/1000 - Score : 406\n",
      "Episode 803/1000 - Score : 231\n",
      "Episode 806/1000 - Score : 379\n",
      "Episode 807/1000 - Score : 227\n",
      "Episode 812/1000 - Score : 269\n",
      "Episode 814/1000 - Score : 476\n",
      "Episode 816/1000 - Score : 433\n",
      "Episode 818/1000 - Score : 254\n",
      "Episode 819/1000 - Score : 341\n",
      "Episode 826/1000 - Score : 300\n",
      "Episode 827/1000 - Score : 373\n",
      "Episode 828/1000 - Score : 493\n",
      "Episode 829/1000 - Score : 448\n",
      "Episode 830/1000 - Score : 316\n",
      "Episode 831/1000 - Score : 253\n",
      "Episode 834/1000 - Score : 322\n",
      "Episode 836/1000 - Score : 133\n",
      "Episode 842/1000 - Score : 481\n",
      "Episode 844/1000 - Score : 139\n",
      "Episode 845/1000 - Score : 141\n",
      "Episode 847/1000 - Score : 475\n",
      "Episode 848/1000 - Score : 490\n",
      "Episode 850/1000 - Score : 415\n",
      "Episode 851/1000 - Score : 285\n",
      "Episode 853/1000 - Score : 342\n",
      "Episode 855/1000 - Score : 326\n",
      "Episode 856/1000 - Score : 345\n",
      "Episode 858/1000 - Score : 322\n",
      "Episode 859/1000 - Score : 219\n",
      "Episode 860/1000 - Score : 211\n",
      "Episode 865/1000 - Score : 177\n",
      "Episode 870/1000 - Score : 309\n",
      "Episode 872/1000 - Score : 271\n",
      "Episode 875/1000 - Score : 221\n",
      "Episode 877/1000 - Score : 294\n",
      "Episode 878/1000 - Score : 462\n",
      "Episode 879/1000 - Score : 245\n",
      "Episode 884/1000 - Score : 155\n",
      "Episode 886/1000 - Score : 196\n",
      "Episode 888/1000 - Score : 436\n",
      "Episode 890/1000 - Score : 485\n",
      "Episode 891/1000 - Score : 126\n",
      "Episode 894/1000 - Score : 287\n",
      "Episode 896/1000 - Score : 122\n",
      "Episode 899/1000 - Score : 426\n",
      "Episode 900/1000 - Score : 306\n",
      "Episode 901/1000 - Score : 241\n",
      "Episode 904/1000 - Score : 214\n",
      "Episode 909/1000 - Score : 254\n",
      "Episode 913/1000 - Score : 357\n",
      "Episode 914/1000 - Score : 318\n",
      "Episode 923/1000 - Score : 303\n",
      "Episode 924/1000 - Score : 58\n",
      "Episode 925/1000 - Score : 491\n",
      "Episode 928/1000 - Score : 416\n",
      "Episode 929/1000 - Score : 121\n",
      "Episode 930/1000 - Score : 452\n",
      "Episode 931/1000 - Score : 163\n",
      "Episode 934/1000 - Score : 468\n",
      "Episode 937/1000 - Score : 392\n",
      "Episode 938/1000 - Score : 215\n",
      "Episode 939/1000 - Score : 472\n",
      "Episode 940/1000 - Score : 473\n",
      "Episode 941/1000 - Score : 274\n",
      "Episode 944/1000 - Score : 309\n",
      "Episode 945/1000 - Score : 238\n",
      "Episode 946/1000 - Score : 175\n",
      "Episode 950/1000 - Score : 407\n",
      "Episode 952/1000 - Score : 236\n",
      "Episode 957/1000 - Score : 278\n",
      "Episode 959/1000 - Score : 323\n",
      "Episode 960/1000 - Score : 199\n",
      "Episode 961/1000 - Score : 205\n",
      "Episode 962/1000 - Score : 464\n",
      "Episode 963/1000 - Score : 252\n",
      "Episode 964/1000 - Score : 339\n",
      "Episode 966/1000 - Score : 359\n",
      "Episode 968/1000 - Score : 330\n",
      "Episode 970/1000 - Score : 320\n",
      "Episode 971/1000 - Score : 247\n",
      "Episode 974/1000 - Score : 402\n",
      "Episode 976/1000 - Score : 491\n",
      "Episode 980/1000 - Score : 111\n",
      "Episode 982/1000 - Score : 487\n",
      "Episode 983/1000 - Score : 286\n",
      "Episode 986/1000 - Score : 283\n",
      "Episode 987/1000 - Score : 492\n",
      "Episode 991/1000 - Score : 230\n",
      "Episode 992/1000 - Score : 331\n",
      "Episode 993/1000 - Score : 242\n",
      "Episode 994/1000 - Score : 98\n",
      "Episode 996/1000 - Score : 309\n",
      "Episode 997/1000 - Score : 450\n",
      "Episode 999/1000 - Score : 225\n"
     ]
    }
   ],
   "source": [
    "env = SnakeEnv()  # Remplace par ton environnement Snake\n",
    "agent = DQN(env)\n",
    "\n",
    "episodes = 1000 # Nombre d'épisodes d'entraînement\n",
    "batch_size = 32  # Taille des lots pour l'entraînement\n",
    "\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, 4])\n",
    "\n",
    "    for time in range(500):  # Limite de temps pour chaque épisode\n",
    "        action = agent.act(state)  # Choisir une action\n",
    "        next_state, reward, done = env.step(action)  # Exécuter l'action\n",
    "        reward = reward if not done else -10  # Pénalité en cas de défaite\n",
    "        next_state = np.reshape(next_state, [1, 4])\n",
    "        \n",
    "        agent.remember(state, action, reward, next_state, done)  # Mémoriser la transition\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            print(f\"Episode {e}/{episodes} - Score : {time}\")\n",
    "            break\n",
    "        \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.replay()  # Entraîner le modèle avec un batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;66;03m# Fermer Pygame correctement\u001b[39;00m\n\u001b[0;32m     33\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mquit()  \u001b[38;5;66;03m# Ajoutez cette ligne pour fermer Pygame\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m \u001b[43mplay_snake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrender\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mplay_snake\u001b[1;34m(env, agent, episodes, render)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m render:\n\u001b[1;32m---> 14\u001b[0m         \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Affiche le jeu à chaque étape\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;66;03m# L'agent choisit une action\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mact(state)\n",
      "Cell \u001b[1;32mIn[2], line 97\u001b[0m, in \u001b[0;36mSnakeEnv.render\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     94\u001b[0m     pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfenetre, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m255\u001b[39m), [bloc[\u001b[38;5;241m0\u001b[39m], bloc[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaille_bloc, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtaille_bloc])\n\u001b[0;32m     96\u001b[0m pygame\u001b[38;5;241m.\u001b[39mdisplay\u001b[38;5;241m.\u001b[39mupdate()\n\u001b[1;32m---> 97\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhorloge\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvitesse_serpent\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pygame  # Assurez-vous d'importer pygame\n",
    "\n",
    "def play_snake(env, agent, episodes=1, render=True):\n",
    "    \"\"\"Fait jouer l'agent au jeu Snake.\"\"\"\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()  # Réinitialiser l'environnement\n",
    "        state = np.reshape(state, [1, 4])  # Ajuster la forme de l'état\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            if render:\n",
    "                env.render()  # Affiche le jeu à chaque étape\n",
    "            \n",
    "            # L'agent choisit une action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Exécute cette action dans l'environnement\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, 4])\n",
    "            \n",
    "            # Passe à l'état suivant\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            \n",
    "            # Si le jeu est terminé (le serpent est mort)\n",
    "            if done:\n",
    "                print(f\"Episode: {episode+1}, Score: {total_reward}\")\n",
    "                break\n",
    "\n",
    "    # Fermer Pygame correctement\n",
    "    pygame.quit()  # Ajoutez cette ligne pour fermer Pygame\n",
    "\n",
    "\n",
    "play_snake(env, agent, episodes=5, render=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
