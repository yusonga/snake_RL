{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snake deep q-learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SONG\\AppData\\Local\\Temp\\ipykernel_16668\\470691393.py:175: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  current_state = torch.tensor(current_state, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 Score 0 Record: 0 Epsilon: 80\n",
      "Game 2 Score 0 Record: 0 Epsilon: 79\n",
      "Game 3 Score 0 Record: 0 Epsilon: 78\n",
      "Game 4 Score 0 Record: 0 Epsilon: 77\n",
      "Game 5 Score 0 Record: 0 Epsilon: 76\n",
      "Game 6 Score 0 Record: 0 Epsilon: 75\n",
      "Game 7 Score 0 Record: 0 Epsilon: 74\n",
      "Game 8 Score 0 Record: 0 Epsilon: 73\n",
      "Game 9 Score 0 Record: 0 Epsilon: 72\n",
      "Game 10 Score 0 Record: 0 Epsilon: 71\n",
      "Game 11 Score 0 Record: 0 Epsilon: 70\n",
      "Game 12 Score 0 Record: 0 Epsilon: 69\n",
      "Game 13 Score 0 Record: 0 Epsilon: 68\n",
      "Game 14 Score 0 Record: 0 Epsilon: 67\n",
      "Game 15 Score 0 Record: 0 Epsilon: 66\n",
      "Game 16 Score 0 Record: 0 Epsilon: 65\n",
      "Game 17 Score 0 Record: 0 Epsilon: 64\n",
      "Game 18 Score 0 Record: 0 Epsilon: 63\n",
      "Game 19 Score 1 Record: 1 Epsilon: 62\n",
      "Game 20 Score 1 Record: 1 Epsilon: 61\n",
      "Game 21 Score 0 Record: 1 Epsilon: 60\n",
      "Game 22 Score 0 Record: 1 Epsilon: 59\n",
      "Game 23 Score 0 Record: 1 Epsilon: 58\n",
      "Game 24 Score 0 Record: 1 Epsilon: 57\n",
      "Game 25 Score 0 Record: 1 Epsilon: 56\n",
      "Game 26 Score 0 Record: 1 Epsilon: 55\n",
      "Game 27 Score 0 Record: 1 Epsilon: 54\n",
      "Game 28 Score 1 Record: 1 Epsilon: 53\n",
      "Game 29 Score 1 Record: 1 Epsilon: 52\n",
      "Game 30 Score 0 Record: 1 Epsilon: 51\n",
      "Game 31 Score 1 Record: 1 Epsilon: 50\n",
      "Game 32 Score 1 Record: 1 Epsilon: 49\n",
      "Game 33 Score 0 Record: 1 Epsilon: 48\n",
      "Game 34 Score 0 Record: 1 Epsilon: 47\n",
      "Game 35 Score 0 Record: 1 Epsilon: 46\n",
      "Game 36 Score 1 Record: 1 Epsilon: 45\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 305\u001b[0m\n\u001b[0;32m    302\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGame\u001b[39m\u001b[38;5;124m'\u001b[39m, agent\u001b[38;5;241m.\u001b[39mgames_played, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore\u001b[39m\u001b[38;5;124m'\u001b[39m, score, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRecord:\u001b[39m\u001b[38;5;124m'\u001b[39m, record, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpsilon:\u001b[39m\u001b[38;5;124m'\u001b[39m, agent\u001b[38;5;241m.\u001b[39mepsilon)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 305\u001b[0m     \u001b[43mplay_snake_ai\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 288\u001b[0m, in \u001b[0;36mplay_snake_ai\u001b[1;34m()\u001b[0m\n\u001b[0;32m    286\u001b[0m state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mextract_state(game)\n\u001b[0;32m    287\u001b[0m move \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mdecide_action(state)\n\u001b[1;32m--> 288\u001b[0m reward, done, score \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_turn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmove\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    289\u001b[0m new_state \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mextract_state(game)\n\u001b[0;32m    290\u001b[0m agent\u001b[38;5;241m.\u001b[39mtrain_short_term(state, move, reward, new_state, done)\n",
      "Cell \u001b[1;32mIn[3], line 94\u001b[0m, in \u001b[0;36mSnakeAI.play_turn\u001b[1;34m(self, choice)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake_body\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Refresh display\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrefresh_display\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mticker\u001b[38;5;241m.\u001b[39mtick(GAME_SPEED)\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reward, game_end, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscore\n",
      "Cell \u001b[1;32mIn[3], line 112\u001b[0m, in \u001b[0;36mSnakeAI.refresh_display\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrefresh_display\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscreen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCOL_BLACK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake_body:\n\u001b[0;32m    115\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscreen, COL_BLUE_PRIMARY, pygame\u001b[38;5;241m.\u001b[39mRect(part\u001b[38;5;241m.\u001b[39mx, part\u001b[38;5;241m.\u001b[39my, TILE_SIZE, TILE_SIZE))\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import deque, namedtuple\n",
    "import pygame\n",
    "from enum import Enum\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "# Initialize pygame\n",
    "pygame.init()\n",
    "# Define font style\n",
    "font_style = pygame.font.SysFont(\"bahnschrift\", 25)\n",
    "\n",
    "class MoveDirection(Enum):\n",
    "    EAST = 1\n",
    "    WEST = 2\n",
    "    NORTH = 3\n",
    "    SOUTH = 4\n",
    "\n",
    "Coordinate = namedtuple('Coordinate', 'x, y')\n",
    "\n",
    "# RGB color definitions\n",
    "COL_WHITE = (255, 255, 255)\n",
    "COL_RED = (200, 0, 0)\n",
    "COL_BLUE_PRIMARY = (0, 0, 255)\n",
    "COL_BLUE_SECONDARY = (0, 100, 255)\n",
    "COL_BLACK = (0, 0, 0)\n",
    "\n",
    "TILE_SIZE = 20\n",
    "GAME_SPEED = 40\n",
    "\n",
    "class SnakeAI:\n",
    "\n",
    "    def __init__(self, width=640, height=480):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.screen = pygame.display.set_mode((self.width, self.height))\n",
    "        pygame.display.set_caption('Snake AI')\n",
    "        self.ticker = pygame.time.Clock()\n",
    "        self.reset_game()\n",
    "\n",
    "    def reset_game(self):\n",
    "        self.heading = MoveDirection.EAST\n",
    "        self.head = Coordinate(self.width / 2, self.height / 2)\n",
    "        self.snake_body = [self.head,\n",
    "                           Coordinate(self.head.x - TILE_SIZE, self.head.y),\n",
    "                           Coordinate(self.head.x - 2 * TILE_SIZE, self.head.y)]\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self.place_food()\n",
    "        self.iterations = 0\n",
    "\n",
    "    def place_food(self):\n",
    "        x = random.randint(0, (self.width - TILE_SIZE) // TILE_SIZE) * TILE_SIZE\n",
    "        y = random.randint(0, (self.height - TILE_SIZE) // TILE_SIZE) * TILE_SIZE\n",
    "        self.food = Coordinate(x, y)\n",
    "        if self.food in self.snake_body:\n",
    "            self.place_food()\n",
    "\n",
    "    def play_turn(self, choice):\n",
    "        self.iterations += 1\n",
    "\n",
    "        # Check for game quit event\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "\n",
    "        # Move snake\n",
    "        self.perform_move(choice)\n",
    "        self.snake_body.insert(0, self.head)\n",
    "\n",
    "        # Check for collision or timeout\n",
    "        reward = 0\n",
    "        game_end = False\n",
    "        if self.check_collision() or self.iterations > 100 * len(self.snake_body):\n",
    "            game_end = True\n",
    "            reward = -10\n",
    "            return reward, game_end, self.score\n",
    "\n",
    "        # Check for food consumption\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self.place_food()\n",
    "        else:\n",
    "            self.snake_body.pop()\n",
    "\n",
    "        # Refresh display\n",
    "        self.refresh_display()\n",
    "        self.ticker.tick(GAME_SPEED)\n",
    "\n",
    "        return reward, game_end, self.score\n",
    "\n",
    "    def check_collision(self, point=None):\n",
    "        if point is None:\n",
    "            point = self.head\n",
    "\n",
    "        # Boundary collision\n",
    "        if point.x > self.width - TILE_SIZE or point.x < 0 or point.y > self.height - TILE_SIZE or point.y < 0:\n",
    "            return True\n",
    "        # Self-collision\n",
    "        if point in self.snake_body[1:]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def refresh_display(self):\n",
    "        self.screen.fill(COL_BLACK)\n",
    "\n",
    "        for part in self.snake_body:\n",
    "            pygame.draw.rect(self.screen, COL_BLUE_PRIMARY, pygame.Rect(part.x, part.y, TILE_SIZE, TILE_SIZE))\n",
    "            pygame.draw.rect(self.screen, COL_BLUE_SECONDARY, pygame.Rect(part.x + 4, part.y + 4, 12, 12))\n",
    "\n",
    "        pygame.draw.rect(self.screen, COL_RED, pygame.Rect(self.food.x, self.food.y, TILE_SIZE, TILE_SIZE))\n",
    "\n",
    "        score_text = font_style.render(\"Score: \" + str(self.score), True, COL_WHITE)\n",
    "        self.screen.blit(score_text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "\n",
    "    def perform_move(self, choice):\n",
    "        directions = [MoveDirection.EAST, MoveDirection.SOUTH, MoveDirection.WEST, MoveDirection.NORTH]\n",
    "        idx = directions.index(self.heading)\n",
    "\n",
    "        if np.array_equal(choice, [1, 0, 0]):\n",
    "            new_heading = directions[idx]  # Keep same direction\n",
    "        elif np.array_equal(choice, [0, 1, 0]):\n",
    "            new_heading = directions[(idx + 1) % 4]  # Right turn\n",
    "        else:  # [0, 0, 1]\n",
    "            new_heading = directions[(idx - 1) % 4]  # Left turn\n",
    "\n",
    "        self.heading = new_heading\n",
    "        x, y = self.head.x, self.head.y\n",
    "        if self.heading == MoveDirection.EAST:\n",
    "            x += TILE_SIZE\n",
    "        elif self.heading == MoveDirection.WEST:\n",
    "            x -= TILE_SIZE\n",
    "        elif self.heading == MoveDirection.SOUTH:\n",
    "            y += TILE_SIZE\n",
    "        elif self.heading == MoveDirection.NORTH:\n",
    "            y -= TILE_SIZE\n",
    "\n",
    "        self.head = Coordinate(x, y)\n",
    "\n",
    "# Deep Q-Learning Model\n",
    "class DeepQNet(nn.Module):\n",
    "    def __init__(self, input_dims, hidden_dims, output_dims):\n",
    "        super(DeepQNet, self).__init__()\n",
    "        self.layer1 = nn.Linear(input_dims, hidden_dims)\n",
    "        self.layer2 = nn.Linear(hidden_dims, output_dims)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, filename='model.pth'):\n",
    "        model_dir = './model'\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        torch.save(self.state_dict(), os.path.join(model_dir, filename))\n",
    "\n",
    "class QLearningTrainer:\n",
    "    def __init__(self, net_model, learning_rate, discount):\n",
    "        self.lr = learning_rate\n",
    "        self.discount = discount\n",
    "        self.model = net_model\n",
    "        self.optimizer = optim.Adam(net_model.parameters(), lr=self.lr)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "    def train_iteration(self, current_state, action_taken, reward_received, next_state, game_done):\n",
    "        current_state = torch.tensor(current_state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action_taken = torch.tensor(action_taken, dtype=torch.long)\n",
    "        reward_received = torch.tensor(reward_received, dtype=torch.float)\n",
    "\n",
    "        if len(current_state.shape) == 1:\n",
    "            current_state = torch.unsqueeze(current_state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action_taken = torch.unsqueeze(action_taken, 0)\n",
    "            reward_received = torch.unsqueeze(reward_received, 0)\n",
    "            game_done = (game_done,)\n",
    "\n",
    "        predictions = self.model(current_state)\n",
    "        target_values = predictions.clone()\n",
    "\n",
    "        for idx in range(len(game_done)):\n",
    "            Q_value = reward_received[idx]\n",
    "            if not game_done[idx]:\n",
    "                Q_value += self.discount * torch.max(self.model(next_state[idx]))\n",
    "            target_values[idx][torch.argmax(action_taken[idx]).item()] = Q_value\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.loss_fn(target_values, predictions)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "MAX_MEM = 100_000\n",
    "BATCH = 1000\n",
    "LEARN_RATE = 0.001\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.games_played = 0\n",
    "        self.epsilon = 0  # randomness factor\n",
    "        self.gamma = 0.9  # discount factor\n",
    "        self.memory = deque(maxlen=MAX_MEM)\n",
    "        self.model = DeepQNet(11, 256, 3)\n",
    "        self.trainer = QLearningTrainer(self.model, learning_rate=LEARN_RATE, discount=self.gamma)\n",
    "\n",
    "    def extract_state(self, game):\n",
    "        head = game.snake_body[0]\n",
    "        left = Coordinate(head.x - TILE_SIZE, head.y)\n",
    "        right = Coordinate(head.x + TILE_SIZE, head.y)\n",
    "        up = Coordinate(head.x, head.y - TILE_SIZE)\n",
    "        down = Coordinate(head.x, head.y + TILE_SIZE)\n",
    "\n",
    "        dir_left = game.heading == MoveDirection.WEST\n",
    "        dir_right = game.heading == MoveDirection.EAST\n",
    "        dir_up = game.heading == MoveDirection.NORTH\n",
    "        dir_down = game.heading == MoveDirection.SOUTH\n",
    "\n",
    "        state = [\n",
    "            (dir_right and game.check_collision(right)) or\n",
    "            (dir_left and game.check_collision(left)) or\n",
    "            (dir_up and game.check_collision(up)) or\n",
    "            (dir_down and game.check_collision(down)),\n",
    "\n",
    "            (dir_up and game.check_collision(right)) or\n",
    "            (dir_down and game.check_collision(left)) or\n",
    "            (dir_left and game.check_collision(up)) or\n",
    "            (dir_right and game.check_collision(down)),\n",
    "\n",
    "            (dir_down and game.check_collision(right)) or\n",
    "            (dir_up and game.check_collision(left)) or\n",
    "            (dir_right and game.check_collision(up)) or\n",
    "            (dir_left and game.check_collision(down)),\n",
    "\n",
    "            dir_left, dir_right, dir_up, dir_down,\n",
    "\n",
    "            game.food.x < game.head.x,\n",
    "            game.food.x > game.head.x,\n",
    "            game.food.y < game.head.y,\n",
    "            game.food.y > game.head.y\n",
    "        ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, game_done):\n",
    "        self.memory.append((state, action, reward, next_state, game_done))\n",
    "\n",
    "    def train_long_term(self):\n",
    "        if len(self.memory) > BATCH:\n",
    "            sample = random.sample(self.memory, BATCH)\n",
    "        else:\n",
    "            sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*sample)\n",
    "        self.trainer.train_iteration(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def train_short_term(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_iteration(state, action, reward, next_state, done)\n",
    "\n",
    "    def decide_action(self, state):\n",
    "        self.epsilon = 80 - self.games_played\n",
    "        final_move = [0, 0, 0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state_tensor)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "\n",
    "def play_snake_ai():\n",
    "    record = 0\n",
    "    agent = DQNAgent()\n",
    "    game = SnakeAI()\n",
    "    while True:\n",
    "        state = agent.extract_state(game)\n",
    "        move = agent.decide_action(state)\n",
    "        reward, done, score = game.play_turn(move)\n",
    "        new_state = agent.extract_state(game)\n",
    "        agent.train_short_term(state, move, reward, new_state, done)\n",
    "        agent.store_experience(state, move, reward, new_state, done)\n",
    "\n",
    "        if done:\n",
    "            game.reset_game()\n",
    "            agent.games_played += 1\n",
    "            agent.train_long_term()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.games_played, 'Score', score, 'Record:', record, 'Epsilon:', agent.epsilon)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    play_snake_ai()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.15)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Episode: 1/1000, Score: 0, Avg Score: 0.00, High Score: 0, Epsilon: 80.0000\n",
      "Episode: 2/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 79.0000\n",
      "Episode: 3/1000, Score: 0, Avg Score: 0.33, High Score: 1, Epsilon: 78.0000\n",
      "Episode: 4/1000, Score: 0, Avg Score: 0.25, High Score: 1, Epsilon: 77.0000\n",
      "Episode: 5/1000, Score: 1, Avg Score: 0.40, High Score: 1, Epsilon: 76.0000\n",
      "Episode: 6/1000, Score: 0, Avg Score: 0.33, High Score: 1, Epsilon: 75.0000\n",
      "Episode: 7/1000, Score: 0, Avg Score: 0.29, High Score: 1, Epsilon: 74.0000\n",
      "Episode: 8/1000, Score: 0, Avg Score: 0.25, High Score: 1, Epsilon: 73.0000\n",
      "Episode: 9/1000, Score: 0, Avg Score: 0.22, High Score: 1, Epsilon: 72.0000\n",
      "Episode: 10/1000, Score: 1, Avg Score: 0.30, High Score: 1, Epsilon: 71.0000\n",
      "Episode: 11/1000, Score: 1, Avg Score: 0.36, High Score: 1, Epsilon: 70.0000\n",
      "Episode: 12/1000, Score: 0, Avg Score: 0.33, High Score: 1, Epsilon: 69.0000\n",
      "Episode: 13/1000, Score: 0, Avg Score: 0.31, High Score: 1, Epsilon: 68.0000\n",
      "Episode: 14/1000, Score: 1, Avg Score: 0.36, High Score: 1, Epsilon: 67.0000\n",
      "Episode: 15/1000, Score: 0, Avg Score: 0.33, High Score: 1, Epsilon: 66.0000\n",
      "Episode: 16/1000, Score: 1, Avg Score: 0.38, High Score: 1, Epsilon: 65.0000\n",
      "Episode: 17/1000, Score: 1, Avg Score: 0.41, High Score: 1, Epsilon: 64.0000\n",
      "Episode: 18/1000, Score: 0, Avg Score: 0.39, High Score: 1, Epsilon: 63.0000\n",
      "Episode: 19/1000, Score: 0, Avg Score: 0.37, High Score: 1, Epsilon: 62.0000\n",
      "Episode: 20/1000, Score: 1, Avg Score: 0.40, High Score: 1, Epsilon: 61.0000\n",
      "Episode: 21/1000, Score: 1, Avg Score: 0.43, High Score: 1, Epsilon: 60.0000\n",
      "Episode: 22/1000, Score: 0, Avg Score: 0.41, High Score: 1, Epsilon: 59.0000\n",
      "Episode: 23/1000, Score: 1, Avg Score: 0.43, High Score: 1, Epsilon: 58.0000\n",
      "Episode: 24/1000, Score: 1, Avg Score: 0.46, High Score: 1, Epsilon: 57.0000\n",
      "Episode: 25/1000, Score: 1, Avg Score: 0.48, High Score: 1, Epsilon: 56.0000\n",
      "Episode: 26/1000, Score: 0, Avg Score: 0.46, High Score: 1, Epsilon: 55.0000\n",
      "Episode: 27/1000, Score: 0, Avg Score: 0.44, High Score: 1, Epsilon: 54.0000\n",
      "Episode: 28/1000, Score: 0, Avg Score: 0.43, High Score: 1, Epsilon: 53.0000\n",
      "Episode: 29/1000, Score: 1, Avg Score: 0.45, High Score: 1, Epsilon: 52.0000\n",
      "Episode: 30/1000, Score: 1, Avg Score: 0.47, High Score: 1, Epsilon: 51.0000\n",
      "Episode: 31/1000, Score: 1, Avg Score: 0.48, High Score: 1, Epsilon: 50.0000\n",
      "Episode: 32/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 49.0000\n",
      "Episode: 33/1000, Score: 0, Avg Score: 0.48, High Score: 1, Epsilon: 48.0000\n",
      "Episode: 34/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 47.0000\n",
      "Episode: 35/1000, Score: 0, Avg Score: 0.49, High Score: 1, Epsilon: 46.0000\n",
      "Episode: 36/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 45.0000\n",
      "Episode: 37/1000, Score: 0, Avg Score: 0.49, High Score: 1, Epsilon: 44.0000\n",
      "Episode: 38/1000, Score: 1, Avg Score: 0.50, High Score: 1, Epsilon: 43.0000\n",
      "Episode: 39/1000, Score: 1, Avg Score: 0.51, High Score: 1, Epsilon: 42.0000\n",
      "Episode: 40/1000, Score: 1, Avg Score: 0.53, High Score: 1, Epsilon: 41.0000\n",
      "Episode: 41/1000, Score: 1, Avg Score: 0.54, High Score: 1, Epsilon: 40.0000\n",
      "Episode: 42/1000, Score: 1, Avg Score: 0.55, High Score: 1, Epsilon: 39.0000\n",
      "Episode: 43/1000, Score: 1, Avg Score: 0.56, High Score: 1, Epsilon: 38.0000\n",
      "Episode: 44/1000, Score: 1, Avg Score: 0.57, High Score: 1, Epsilon: 37.0000\n",
      "Episode: 45/1000, Score: 1, Avg Score: 0.58, High Score: 1, Epsilon: 36.0000\n",
      "Episode: 46/1000, Score: 0, Avg Score: 0.57, High Score: 1, Epsilon: 35.0000\n",
      "Episode: 47/1000, Score: 1, Avg Score: 0.57, High Score: 1, Epsilon: 34.0000\n",
      "Episode: 48/1000, Score: 1, Avg Score: 0.58, High Score: 1, Epsilon: 33.0000\n",
      "Episode: 49/1000, Score: 1, Avg Score: 0.59, High Score: 1, Epsilon: 32.0000\n",
      "Episode: 50/1000, Score: 1, Avg Score: 0.60, High Score: 1, Epsilon: 31.0000\n",
      "Episode: 51/1000, Score: 1, Avg Score: 0.61, High Score: 1, Epsilon: 30.0000\n",
      "Episode: 52/1000, Score: 1, Avg Score: 0.62, High Score: 1, Epsilon: 29.0000\n",
      "Episode: 53/1000, Score: 1, Avg Score: 0.62, High Score: 1, Epsilon: 28.0000\n",
      "Episode: 54/1000, Score: 1, Avg Score: 0.63, High Score: 1, Epsilon: 27.0000\n",
      "Episode: 55/1000, Score: 1, Avg Score: 0.64, High Score: 1, Epsilon: 26.0000\n",
      "Episode: 56/1000, Score: 0, Avg Score: 0.62, High Score: 1, Epsilon: 25.0000\n",
      "Episode: 57/1000, Score: 1, Avg Score: 0.63, High Score: 1, Epsilon: 24.0000\n",
      "Episode: 58/1000, Score: 1, Avg Score: 0.64, High Score: 1, Epsilon: 23.0000\n",
      "Episode: 59/1000, Score: 1, Avg Score: 0.64, High Score: 1, Epsilon: 22.0000\n",
      "Episode: 60/1000, Score: 0, Avg Score: 0.63, High Score: 1, Epsilon: 21.0000\n",
      "Episode: 61/1000, Score: 2, Avg Score: 0.66, High Score: 2, Epsilon: 20.0000\n",
      "Episode: 62/1000, Score: 1, Avg Score: 0.66, High Score: 2, Epsilon: 19.0000\n",
      "Episode: 63/1000, Score: 1, Avg Score: 0.67, High Score: 2, Epsilon: 18.0000\n",
      "Episode: 64/1000, Score: 1, Avg Score: 0.67, High Score: 2, Epsilon: 17.0000\n",
      "Episode: 65/1000, Score: 1, Avg Score: 0.68, High Score: 2, Epsilon: 16.0000\n",
      "Episode: 66/1000, Score: 1, Avg Score: 0.68, High Score: 2, Epsilon: 15.0000\n",
      "Episode: 67/1000, Score: 1, Avg Score: 0.69, High Score: 2, Epsilon: 14.0000\n",
      "Episode: 68/1000, Score: 1, Avg Score: 0.69, High Score: 2, Epsilon: 13.0000\n",
      "Episode: 69/1000, Score: 0, Avg Score: 0.68, High Score: 2, Epsilon: 12.0000\n",
      "Episode: 70/1000, Score: 1, Avg Score: 0.69, High Score: 2, Epsilon: 11.0000\n",
      "Episode: 71/1000, Score: 0, Avg Score: 0.68, High Score: 2, Epsilon: 10.0000\n",
      "Episode: 72/1000, Score: 0, Avg Score: 0.67, High Score: 2, Epsilon: 9.0000\n",
      "Episode: 73/1000, Score: 3, Avg Score: 0.70, High Score: 3, Epsilon: 8.0000\n",
      "Episode: 74/1000, Score: 1, Avg Score: 0.70, High Score: 3, Epsilon: 7.0000\n",
      "Episode: 75/1000, Score: 1, Avg Score: 0.71, High Score: 3, Epsilon: 6.0000\n",
      "Episode: 76/1000, Score: 0, Avg Score: 0.70, High Score: 3, Epsilon: 5.0000\n",
      "Episode: 77/1000, Score: 1, Avg Score: 0.70, High Score: 3, Epsilon: 4.0000\n",
      "Episode: 78/1000, Score: 2, Avg Score: 0.72, High Score: 3, Epsilon: 3.0000\n",
      "Episode: 79/1000, Score: 1, Avg Score: 0.72, High Score: 3, Epsilon: 2.0000\n",
      "Episode: 80/1000, Score: 1, Avg Score: 0.72, High Score: 3, Epsilon: 1.0000\n",
      "Episode: 81/1000, Score: 1, Avg Score: 0.73, High Score: 3, Epsilon: 0.0000\n",
      "Episode: 82/1000, Score: 1, Avg Score: 0.73, High Score: 3, Epsilon: -1.0000\n",
      "Episode: 83/1000, Score: 1, Avg Score: 0.73, High Score: 3, Epsilon: -2.0000\n",
      "Episode: 84/1000, Score: 2, Avg Score: 0.75, High Score: 3, Epsilon: -3.0000\n",
      "Episode: 85/1000, Score: 1, Avg Score: 0.75, High Score: 3, Epsilon: -4.0000\n",
      "Episode: 86/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -5.0000\n",
      "Episode: 87/1000, Score: 0, Avg Score: 0.75, High Score: 3, Epsilon: -6.0000\n",
      "Episode: 88/1000, Score: 2, Avg Score: 0.76, High Score: 3, Epsilon: -7.0000\n",
      "Episode: 89/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -8.0000\n",
      "Episode: 90/1000, Score: 1, Avg Score: 0.77, High Score: 3, Epsilon: -9.0000\n",
      "Episode: 91/1000, Score: 0, Avg Score: 0.76, High Score: 3, Epsilon: -10.0000\n",
      "Episode: 92/1000, Score: 0, Avg Score: 0.75, High Score: 3, Epsilon: -11.0000\n",
      "Episode: 93/1000, Score: 1, Avg Score: 0.75, High Score: 3, Epsilon: -12.0000\n",
      "Episode: 94/1000, Score: 2, Avg Score: 0.77, High Score: 3, Epsilon: -13.0000\n",
      "Episode: 95/1000, Score: 0, Avg Score: 0.76, High Score: 3, Epsilon: -14.0000\n",
      "Episode: 96/1000, Score: 0, Avg Score: 0.75, High Score: 3, Epsilon: -15.0000\n",
      "Episode: 97/1000, Score: 0, Avg Score: 0.74, High Score: 3, Epsilon: -16.0000\n",
      "Episode: 98/1000, Score: 0, Avg Score: 0.73, High Score: 3, Epsilon: -17.0000\n",
      "Episode: 99/1000, Score: 1, Avg Score: 0.74, High Score: 3, Epsilon: -18.0000\n",
      "Episode: 100/1000, Score: 1, Avg Score: 0.74, High Score: 3, Epsilon: -19.0000\n",
      "Episode: 101/1000, Score: 1, Avg Score: 0.74, High Score: 3, Epsilon: -20.0000\n",
      "Episode: 102/1000, Score: 2, Avg Score: 0.75, High Score: 3, Epsilon: -21.0000\n",
      "Episode: 103/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -22.0000\n",
      "Episode: 104/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -23.0000\n",
      "Episode: 105/1000, Score: 1, Avg Score: 0.76, High Score: 3, Epsilon: -24.0000\n",
      "Episode: 106/1000, Score: 0, Avg Score: 0.75, High Score: 3, Epsilon: -25.0000\n",
      "Episode: 107/1000, Score: 2, Avg Score: 0.77, High Score: 3, Epsilon: -26.0000\n",
      "Episode: 108/1000, Score: 2, Avg Score: 0.78, High Score: 3, Epsilon: -27.0000\n",
      "Episode: 109/1000, Score: 1, Avg Score: 0.78, High Score: 3, Epsilon: -28.0000\n",
      "Episode: 110/1000, Score: 1, Avg Score: 0.78, High Score: 3, Epsilon: -29.0000\n",
      "Episode: 111/1000, Score: 1, Avg Score: 0.78, High Score: 3, Epsilon: -30.0000\n",
      "Episode: 112/1000, Score: 1, Avg Score: 0.79, High Score: 3, Epsilon: -31.0000\n",
      "Episode: 113/1000, Score: 2, Avg Score: 0.80, High Score: 3, Epsilon: -32.0000\n",
      "Episode: 114/1000, Score: 1, Avg Score: 0.80, High Score: 3, Epsilon: -33.0000\n",
      "Episode: 115/1000, Score: 1, Avg Score: 0.80, High Score: 3, Epsilon: -34.0000\n",
      "Episode: 116/1000, Score: 1, Avg Score: 0.80, High Score: 3, Epsilon: -35.0000\n",
      "Episode: 117/1000, Score: 1, Avg Score: 0.80, High Score: 3, Epsilon: -36.0000\n",
      "Episode: 118/1000, Score: 2, Avg Score: 0.81, High Score: 3, Epsilon: -37.0000\n",
      "Episode: 119/1000, Score: 3, Avg Score: 0.83, High Score: 3, Epsilon: -38.0000\n",
      "Episode: 120/1000, Score: 1, Avg Score: 0.83, High Score: 3, Epsilon: -39.0000\n",
      "Episode: 121/1000, Score: 2, Avg Score: 0.84, High Score: 3, Epsilon: -40.0000\n",
      "Episode: 122/1000, Score: 1, Avg Score: 0.84, High Score: 3, Epsilon: -41.0000\n",
      "Episode: 123/1000, Score: 1, Avg Score: 0.85, High Score: 3, Epsilon: -42.0000\n",
      "Episode: 124/1000, Score: 1, Avg Score: 0.85, High Score: 3, Epsilon: -43.0000\n",
      "Episode: 125/1000, Score: 1, Avg Score: 0.85, High Score: 3, Epsilon: -44.0000\n",
      "Episode: 126/1000, Score: 1, Avg Score: 0.85, High Score: 3, Epsilon: -45.0000\n",
      "Episode: 127/1000, Score: 3, Avg Score: 0.87, High Score: 3, Epsilon: -46.0000\n",
      "Episode: 128/1000, Score: 1, Avg Score: 0.87, High Score: 3, Epsilon: -47.0000\n",
      "Episode: 129/1000, Score: 1, Avg Score: 0.87, High Score: 3, Epsilon: -48.0000\n",
      "Episode: 130/1000, Score: 1, Avg Score: 0.87, High Score: 3, Epsilon: -49.0000\n",
      "Episode: 131/1000, Score: 4, Avg Score: 0.89, High Score: 4, Epsilon: -50.0000\n",
      "Episode: 132/1000, Score: 1, Avg Score: 0.89, High Score: 4, Epsilon: -51.0000\n",
      "Episode: 133/1000, Score: 1, Avg Score: 0.89, High Score: 4, Epsilon: -52.0000\n",
      "Episode: 134/1000, Score: 3, Avg Score: 0.91, High Score: 4, Epsilon: -53.0000\n",
      "Episode: 135/1000, Score: 2, Avg Score: 0.92, High Score: 4, Epsilon: -54.0000\n",
      "Episode: 136/1000, Score: 2, Avg Score: 0.93, High Score: 4, Epsilon: -55.0000\n",
      "Episode: 137/1000, Score: 1, Avg Score: 0.93, High Score: 4, Epsilon: -56.0000\n",
      "Episode: 138/1000, Score: 2, Avg Score: 0.93, High Score: 4, Epsilon: -57.0000\n",
      "Episode: 139/1000, Score: 1, Avg Score: 0.94, High Score: 4, Epsilon: -58.0000\n",
      "Episode: 140/1000, Score: 1, Avg Score: 0.94, High Score: 4, Epsilon: -59.0000\n",
      "Episode: 141/1000, Score: 1, Avg Score: 0.94, High Score: 4, Epsilon: -60.0000\n",
      "Episode: 142/1000, Score: 1, Avg Score: 0.94, High Score: 4, Epsilon: -61.0000\n",
      "Episode: 143/1000, Score: 2, Avg Score: 0.94, High Score: 4, Epsilon: -62.0000\n",
      "Episode: 144/1000, Score: 2, Avg Score: 0.95, High Score: 4, Epsilon: -63.0000\n",
      "Episode: 145/1000, Score: 2, Avg Score: 0.96, High Score: 4, Epsilon: -64.0000\n",
      "Episode: 146/1000, Score: 2, Avg Score: 0.97, High Score: 4, Epsilon: -65.0000\n",
      "Episode: 147/1000, Score: 1, Avg Score: 0.97, High Score: 4, Epsilon: -66.0000\n",
      "Episode: 148/1000, Score: 1, Avg Score: 0.97, High Score: 4, Epsilon: -67.0000\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#### final one!!!! some adjustment but works quite well than before!\n",
    "\n",
    "import pygame\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "# Pygame setup\n",
    "pygame.init()\n",
    "# Colors and window dimensions\n",
    "blanc, jaune, noir, rouge, vert, bleu = (255, 255, 255), (255, 255, 102), (0, 0, 0), (213, 50, 80), (0, 255, 0), (50, 153, 213)\n",
    "largeur_ecran, hauteur_ecran, taille_bloc, vitesse_serpent = 600, 400, 20, 40\n",
    "fenetre = pygame.display.set_mode((largeur_ecran, hauteur_ecran))\n",
    "pygame.display.set_caption(\"Snake Game - DQN\")\n",
    "horloge = pygame.time.Clock()\n",
    "police = pygame.font.SysFont(\"bahnschrift\", 25)\n",
    "\n",
    "# Display score\n",
    "def afficher_score(score):\n",
    "    valeur = police.render(\"Score: \" + str(score), True, jaune)\n",
    "    fenetre.blit(valeur, [0, 0])\n",
    "\n",
    "# Neural Network for DQN with two selectable architectures\n",
    "\n",
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, complex_model=False):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Define network architecture based on model complexity\n",
    "        if complex_model:\n",
    "            # Complex model with two hidden layers\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, hidden_size * 2)\n",
    "            self.fc3 = nn.Linear(hidden_size * 2, output_size)\n",
    "            self.complex = True\n",
    "        else:\n",
    "            # Simple model with one hidden layer\n",
    "            self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "            self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "            self.complex = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer with ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        \n",
    "        # Pass through additional layer if complex model\n",
    "        if self.complex:\n",
    "            x = F.relu(self.fc2(x))\n",
    "            x = self.fc3(x)  # Output layer\n",
    "        else:\n",
    "            x = self.fc2(x)  # Output layer for simple model\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        # Save model weights to specified directory\n",
    "        model_folder_path = './models'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "        \n",
    "        file_path = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_path)\n",
    "\n",
    "# Q-learning Trainer Class\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.model = model\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # Convert inputs to tensors, adding batch dimension if needed\n",
    "        state = torch.FloatTensor(state).unsqueeze(0) if len(state.shape) == 1 else torch.FloatTensor(state)\n",
    "        next_state = torch.FloatTensor(next_state).unsqueeze(0) if len(next_state.shape) == 1 else torch.FloatTensor(next_state)\n",
    "        action = torch.LongTensor([action]) if isinstance(action, int) else torch.LongTensor(action)\n",
    "        reward = torch.FloatTensor([reward]) if isinstance(reward, (int, float)) else torch.FloatTensor(reward)\n",
    "        done = torch.FloatTensor([done]) if isinstance(done, bool) else torch.FloatTensor(done)\n",
    "\n",
    "        # Predicted Q values for current state-action pairs\n",
    "        pred = self.model(state).gather(1, action.view(-1, 1))\n",
    "\n",
    "        # Calculate target Q values\n",
    "        with torch.no_grad():\n",
    "            next_q_values = self.model(next_state).max(1)[0]\n",
    "            target_q_values = reward + (1 - done) * self.gamma * next_q_values\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = self.criterion(pred, target_q_values.unsqueeze(1))\n",
    "\n",
    "        # Backpropagation\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "# Paramètres pour le DQN (modifiés)\n",
    "HIDDEN_SIZE = 128  # Increased hidden size\n",
    "OUTPUT_SIZE = 4\n",
    "BATCH_SIZE = 64  # Increased batch size\n",
    "MEMORY_SIZE = 200000  # Increased memory size\n",
    "UPDATE_TARGET_EVERY = 4  # Update target network less frequently\n",
    "START_TRAINING_THRESHOLD = BATCH_SIZE * 10\n",
    "MAX_EPISODES = 1000\n",
    "# Agent with model, training, and actions\n",
    "class AIAgent:\n",
    "    def __init__(self, input_size, hidden_size, output_size, gamma=0.99, lr=0.001, epsilon_start=1.0, epsilon_end=0.01, epsilon_decay=0.9, complex_model=False, max_episodes=1000, batch_size=128):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = epsilon_start\n",
    "        self.epsilon_start = epsilon_start\n",
    "        self.epsilon_end = epsilon_end\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.gamma = gamma\n",
    "        self.memory = deque(maxlen=200000)\n",
    "        \n",
    "        # Model initialization\n",
    "        self.model = Linear_QNet(input_size, hidden_size, output_size)\n",
    "        self.target_model = Linear_QNet(input_size, hidden_size, output_size)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "        self.trainer = QTrainer(self.model, lr=lr, gamma=gamma)\n",
    "        \n",
    "        # Tracking episodes and batch size for training\n",
    "        self.games_played = 0\n",
    "        self.episode_memory = []\n",
    "        self.max_episodes = max_episodes\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def get_state(self, snake, food):\n",
    "        head = snake[-1]\n",
    "        point_l = [head[0] - taille_bloc, head[1]]\n",
    "        point_r = [head[0] + taille_bloc, head[1]]\n",
    "        point_u = [head[0], head[1] - taille_bloc]\n",
    "        point_d = [head[0], head[1] + taille_bloc]\n",
    "        \n",
    "        dir_l = head[0] > snake[-2][0] if len(snake) > 1 else False\n",
    "        dir_r = head[0] < snake[-2][0] if len(snake) > 1 else False\n",
    "        dir_u = head[1] > snake[-2][1] if len(snake) > 1 else False\n",
    "        dir_d = head[1] < snake[-2][1] if len(snake) > 1 else False\n",
    "\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and self.is_collision(point_r, snake)) or \n",
    "            (dir_l and self.is_collision(point_l, snake)) or \n",
    "            (dir_u and self.is_collision(point_u, snake)) or \n",
    "            (dir_d and self.is_collision(point_d, snake)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and self.is_collision(point_r, snake)) or \n",
    "            (dir_d and self.is_collision(point_l, snake)) or \n",
    "            (dir_l and self.is_collision(point_u, snake)) or \n",
    "            (dir_r and self.is_collision(point_d, snake)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and self.is_collision(point_r, snake)) or \n",
    "            (dir_u and self.is_collision(point_l, snake)) or \n",
    "            (dir_r and self.is_collision(point_u, snake)) or \n",
    "            (dir_l and self.is_collision(point_d, snake)),\n",
    "            \n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "            \n",
    "            # Food location \n",
    "            food[0] < head[0],  # food left\n",
    "            food[0] > head[0],  # food right\n",
    "            food[1] < head[1],  # food up\n",
    "            food[1] > head[1]   # food down\n",
    "        ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def is_collision(self, point, snake):\n",
    "        if point[0] < 0 or point[0] >= largeur_ecran or point[1] < 0 or point[1] >= hauteur_ecran:\n",
    "            return True\n",
    "        if point in snake[:-1]:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        \n",
    "        # Train on batch\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        self.epsilon = 80 - self.games_played\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            return random.randint(0, 2)\n",
    "        else:\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state_tensor)\n",
    "            return torch.argmax(prediction).item()\n",
    "\n",
    "   \n",
    "    def store_episode(self, state, action, reward, next_state, done):\n",
    "        self.episode_memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_on_episode(self):\n",
    "        for experience in self.episode_memory:\n",
    "            self.remember(*experience)\n",
    "        \n",
    "        # Train only if enough memory is stored\n",
    "        if len(self.memory) >= self.batch_size:\n",
    "            self.train_long_memory()\n",
    "        \n",
    "        # Clear episode memory after training\n",
    "        self.episode_memory.clear()\n",
    "\n",
    "# RGB color values\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (200,0,0)\n",
    "BLUE1 = (0, 0, 255)\n",
    "BLUE2 = (0, 100, 255)\n",
    "BLACK = (0,0,0)\n",
    "def jeu():\n",
    "    agent = AIAgent(11, HIDDEN_SIZE, OUTPUT_SIZE, complex_model=True)  \n",
    "    nb_episodes, total_score, high_score = 0, 0, 0\n",
    "    scores, avg_scores = [], []\n",
    "    \n",
    "    while nb_episodes < MAX_EPISODES:\n",
    "        game_over = False\n",
    "        score = 0\n",
    "        serpent = [[largeur_ecran / 2, hauteur_ecran / 2]]\n",
    "        x_nourriture, y_nourriture = round(random.randrange(0, largeur_ecran - taille_bloc) / taille_bloc) * taille_bloc, round(random.randrange(0, hauteur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "        steps_without_food = 0\n",
    "\n",
    "        while not game_over:\n",
    "            # Get the state and decide on an action\n",
    "            state = agent.get_state(serpent, (x_nourriture, y_nourriture))\n",
    "            action = agent.get_action(state)\n",
    "\n",
    "            # Move snake based on action\n",
    "            x_change, y_change = [(0, -taille_bloc), (0, taille_bloc), (-taille_bloc, 0), (taille_bloc, 0)][action]\n",
    "            nouvelle_tete = [serpent[-1][0] + x_change, serpent[-1][1] + y_change]\n",
    "            serpent.append(nouvelle_tete)\n",
    "\n",
    "            # Check for game over\n",
    "            if (nouvelle_tete[0] < 0 or nouvelle_tete[0] >= largeur_ecran or\n",
    "                nouvelle_tete[1] < 0 or nouvelle_tete[1] >= hauteur_ecran or\n",
    "                nouvelle_tete in serpent[:-1]):\n",
    "                game_over = True\n",
    "                reward = -10\n",
    "            else:\n",
    "                reward = -0.01  # Small survival reward\n",
    "\n",
    "                # Check for food consumption\n",
    "                if nouvelle_tete[0] == x_nourriture and nouvelle_tete[1] == y_nourriture:\n",
    "                    reward = 50  # Eating food reward\n",
    "                    score += 1\n",
    "                    x_nourriture = round(random.randrange(0, largeur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "                    y_nourriture = round(random.randrange(0, hauteur_ecran - taille_bloc) / taille_bloc) * taille_bloc\n",
    "                    steps_without_food = 0\n",
    "                else:\n",
    "                    serpent.pop(0)  # Snake moves without growing\n",
    "\n",
    "                    # Reward adjustment for distance to food\n",
    "                    if len(serpent) > 1:\n",
    "                        distance_before = np.linalg.norm(np.array(serpent[-2]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    else:\n",
    "                        distance_before = np.linalg.norm(np.array(serpent[-1]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    distance_after = np.linalg.norm(np.array(nouvelle_tete) - np.array([x_nourriture, y_nourriture]))\n",
    "                    reward += 10 if distance_after < distance_before else -0.5\n",
    "                    # distance_before = np.linalg.norm(np.array(serpent[-2]) - np.array([x_nourriture, y_nourriture]))\n",
    "                    # distance_after = np.linalg.norm(np.array(nouvelle_tete) - np.array([x_nourriture, y_nourriture]))\n",
    "                    # reward += 10 if distance_after < distance_before else -0.5\n",
    "                    reward += 0.1  # Additional survival reward\n",
    "\n",
    "                    steps_without_food += 1\n",
    "                    if steps_without_food > 100:  # Penalize long survival without eating\n",
    "                        reward -= 1\n",
    "                        steps_without_food = 0\n",
    "\n",
    "            # Prepare for the next step\n",
    "            next_state = agent.get_state(serpent, (x_nourriture, y_nourriture))\n",
    "            agent.store_episode(state, action, reward, next_state, game_over)\n",
    "\n",
    "            # Update Pygame window\n",
    "            fenetre.fill(BLACK)\n",
    "            pygame.draw.rect(fenetre, RED, [x_nourriture, y_nourriture, taille_bloc, taille_bloc])\n",
    "            for bloc in serpent:\n",
    "                pygame.draw.rect(fenetre, WHITE, [bloc[0], bloc[1], taille_bloc, taille_bloc])\n",
    "            afficher_score(score)\n",
    "            pygame.display.update()\n",
    "            horloge.tick(vitesse_serpent)\n",
    "\n",
    "        # Training and score tracking\n",
    "        agent.train_on_episode()\n",
    "        nb_episodes += 1\n",
    "        agent.games_played+=1\n",
    "        total_score += score\n",
    "        high_score = max(high_score, score)\n",
    "        avg_score = total_score / nb_episodes\n",
    "        scores.append(score)\n",
    "        avg_scores.append(avg_score)\n",
    "\n",
    "        # Update epsilon for exploration-exploitation balance\n",
    "        # agent.update_epsilon(nb_episodes)\n",
    "\n",
    "        print(f\"Episode: {nb_episodes}/{MAX_EPISODES}, Score: {score}, Avg Score: {avg_score:.2f}, High Score: {high_score}, Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "        # Sync target network and save model periodically\n",
    "        if nb_episodes % UPDATE_TARGET_EVERY == 0:\n",
    "            agent.target_model.load_state_dict(agent.model.state_dict())\n",
    "        if nb_episodes % 100 == 0:\n",
    "            agent.model.save(f'snake_dqn_model_episode_{nb_episodes}.pth')\n",
    "    \n",
    "    pygame.quit()\n",
    "    agent.plot_history(scores, avg_scores)\n",
    "    print(f\"Training completed. Total episodes: {nb_episodes}, Final Avg Score: {avg_score:.2f}, High Score: {high_score}\")\n",
    "\n",
    "# Run the game\n",
    "jeu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.9.15)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "import pygame\n",
    "import random\n",
    "from enum import Enum\n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "\n",
    "pygame.init()\n",
    "# font = pygame.font.Font('arial.ttf', 25)\n",
    "#font = pygame.font.SysFont('arial', 25)\n",
    "police = pygame.font.SysFont(\"bahnschrift\", 25)\n",
    "\n",
    "class Direction(Enum):\n",
    "    RIGHT = 1\n",
    "    LEFT = 2\n",
    "    UP = 3\n",
    "    DOWN = 4\n",
    "\n",
    "Point = namedtuple('Point', 'x, y')\n",
    "\n",
    "# rgb colors\n",
    "WHITE = (255, 255, 255)\n",
    "RED = (200,0,0)\n",
    "BLUE1 = (0, 0, 255)\n",
    "BLUE2 = (0, 100, 255)\n",
    "BLACK = (0,0,0)\n",
    "\n",
    "BLOCK_SIZE = 20\n",
    "SPEED = 40\n",
    "\n",
    "class SnakeGameAI:\n",
    "\n",
    "    def __init__(self, w=640, h=480):\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "        # init display\n",
    "        self.display = pygame.display.set_mode((self.w, self.h))\n",
    "        pygame.display.set_caption('Snake')\n",
    "        self.clock = pygame.time.Clock()\n",
    "        self.reset()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        # init game state\n",
    "        self.direction = Direction.RIGHT\n",
    "\n",
    "        self.head = Point(self.w/2, self.h/2)\n",
    "        self.snake = [self.head,\n",
    "                      Point(self.head.x-BLOCK_SIZE, self.head.y),\n",
    "                      Point(self.head.x-(2*BLOCK_SIZE), self.head.y)]\n",
    "\n",
    "        self.score = 0\n",
    "        self.food = None\n",
    "        self._place_food()\n",
    "        self.frame_iteration = 0\n",
    "\n",
    "\n",
    "    def _place_food(self):\n",
    "        x = random.randint(0, (self.w-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        y = random.randint(0, (self.h-BLOCK_SIZE )//BLOCK_SIZE )*BLOCK_SIZE\n",
    "        self.food = Point(x, y)\n",
    "        if self.food in self.snake:\n",
    "            self._place_food()\n",
    "\n",
    "\n",
    "    def play_step(self, action):\n",
    "        self.frame_iteration += 1\n",
    "        # 1. collect user input\n",
    "        for event in pygame.event.get():\n",
    "            if event.type == pygame.QUIT:\n",
    "                pygame.quit()\n",
    "                quit()\n",
    "        \n",
    "        # 2. move\n",
    "        self._move(action) # update the head\n",
    "        self.snake.insert(0, self.head)\n",
    "        \n",
    "        # 3. check if game over\n",
    "        reward = 0\n",
    "        game_over = False\n",
    "        if self.is_collision() or self.frame_iteration > 100*len(self.snake):\n",
    "            game_over = True\n",
    "            reward = -10\n",
    "            return reward, game_over, self.score\n",
    "\n",
    "        # 4. place new food or just move\n",
    "        if self.head == self.food:\n",
    "            self.score += 1\n",
    "            reward = 10\n",
    "            self._place_food()\n",
    "        else:\n",
    "            self.snake.pop()\n",
    "        \n",
    "        # 5. update ui and clock\n",
    "        self._update_ui()\n",
    "        self.clock.tick(SPEED)\n",
    "        # 6. return game over and score\n",
    "        return reward, game_over, self.score\n",
    "\n",
    "\n",
    "    def is_collision(self, pt=None):\n",
    "        if pt is None:\n",
    "            pt = self.head\n",
    "        # hits boundary\n",
    "        if pt.x > self.w - BLOCK_SIZE or pt.x < 0 or pt.y > self.h - BLOCK_SIZE or pt.y < 0:\n",
    "            return True\n",
    "        # hits itself\n",
    "        if pt in self.snake[1:]:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "\n",
    "    def _update_ui(self):\n",
    "        self.display.fill(BLACK)\n",
    "\n",
    "        for pt in self.snake:\n",
    "            pygame.draw.rect(self.display, BLUE1, pygame.Rect(pt.x, pt.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "            pygame.draw.rect(self.display, BLUE2, pygame.Rect(pt.x+4, pt.y+4, 12, 12))\n",
    "\n",
    "        pygame.draw.rect(self.display, RED, pygame.Rect(self.food.x, self.food.y, BLOCK_SIZE, BLOCK_SIZE))\n",
    "\n",
    "        text = police.render(\"Score: \" + str(self.score), True, WHITE)\n",
    "        \n",
    "        self.display.blit(text, [0, 0])\n",
    "        pygame.display.flip()\n",
    "\n",
    "\n",
    "    def _move(self, action):\n",
    "        # [straight, right, left]\n",
    "\n",
    "        clock_wise = [Direction.RIGHT, Direction.DOWN, Direction.LEFT, Direction.UP]\n",
    "        idx = clock_wise.index(self.direction)\n",
    "\n",
    "        if np.array_equal(action, [1, 0, 0]):\n",
    "            new_dir = clock_wise[idx] # no change\n",
    "        elif np.array_equal(action, [0, 1, 0]):\n",
    "            next_idx = (idx + 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # right turn r -> d -> l -> u\n",
    "        else: # [0, 0, 1]\n",
    "            next_idx = (idx - 1) % 4\n",
    "            new_dir = clock_wise[next_idx] # left turn r -> u -> l -> d\n",
    "\n",
    "        self.direction = new_dir\n",
    "\n",
    "        x = self.head.x\n",
    "        y = self.head.y\n",
    "        if self.direction == Direction.RIGHT:\n",
    "            x += BLOCK_SIZE\n",
    "        elif self.direction == Direction.LEFT:\n",
    "            x -= BLOCK_SIZE\n",
    "        elif self.direction == Direction.DOWN:\n",
    "            y += BLOCK_SIZE\n",
    "        elif self.direction == Direction.UP:\n",
    "            y -= BLOCK_SIZE\n",
    "\n",
    "        self.head = Point(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "\n",
    "class Linear_QNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "    def save(self, file_name='model.pth'):\n",
    "        model_folder_path = './model'\n",
    "        if not os.path.exists(model_folder_path):\n",
    "            os.makedirs(model_folder_path)\n",
    "\n",
    "        file_name = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_name)\n",
    "\n",
    "\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        state = torch.tensor(state, dtype=torch.float)\n",
    "        next_state = torch.tensor(next_state, dtype=torch.float)\n",
    "        action = torch.tensor(action, dtype=torch.long)\n",
    "        reward = torch.tensor(reward, dtype=torch.float)\n",
    "        # (n, x)\n",
    "\n",
    "        if len(state.shape) == 1:\n",
    "            # (1, x)\n",
    "            state = torch.unsqueeze(state, 0)\n",
    "            next_state = torch.unsqueeze(next_state, 0)\n",
    "            action = torch.unsqueeze(action, 0)\n",
    "            reward = torch.unsqueeze(reward, 0)\n",
    "            done = (done, )\n",
    "\n",
    "        # 1: predicted Q values with current state\n",
    "        pred = self.model(state)\n",
    "\n",
    "        target = pred.clone()\n",
    "        for idx in range(len(done)):\n",
    "            Q_new = reward[idx]\n",
    "            if not done[idx]:\n",
    "                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))\n",
    "\n",
    "            target[idx][torch.argmax(action[idx]).item()] = Q_new\n",
    "    \n",
    "        # 2: Q_new = r + y * max(next_predicted Q value) -> only do this if not done\n",
    "        # pred.clone()\n",
    "        # preds[argmax(action)] = Q_new\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = self.criterion(target, pred)\n",
    "        loss.backward()\n",
    "\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SONG\\AppData\\Local\\Temp\\ipykernel_12064\\253291230.py:37: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:281.)\n",
      "  state = torch.tensor(state, dtype=torch.float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 Score 0 Record: 0 Epsilon: 80\n",
      "Game 2 Score 0 Record: 0 Epsilon: 79\n",
      "Game 3 Score 0 Record: 0 Epsilon: 78\n",
      "Game 4 Score 0 Record: 0 Epsilon: 77\n",
      "Game 5 Score 0 Record: 0 Epsilon: 76\n",
      "Game 6 Score 0 Record: 0 Epsilon: 75\n",
      "Game 7 Score 1 Record: 1 Epsilon: 74\n",
      "Game 8 Score 0 Record: 1 Epsilon: 73\n",
      "Game 9 Score 0 Record: 1 Epsilon: 72\n",
      "Game 10 Score 0 Record: 1 Epsilon: 71\n",
      "Game 11 Score 0 Record: 1 Epsilon: 70\n",
      "Game 12 Score 1 Record: 1 Epsilon: 69\n",
      "Game 13 Score 1 Record: 1 Epsilon: 68\n",
      "Game 14 Score 0 Record: 1 Epsilon: 67\n",
      "Game 15 Score 1 Record: 1 Epsilon: 66\n",
      "Game 16 Score 0 Record: 1 Epsilon: 65\n",
      "Game 17 Score 0 Record: 1 Epsilon: 64\n",
      "Game 18 Score 1 Record: 1 Epsilon: 63\n",
      "Game 19 Score 1 Record: 1 Epsilon: 62\n",
      "Game 20 Score 1 Record: 1 Epsilon: 61\n",
      "Game 21 Score 1 Record: 1 Epsilon: 60\n",
      "Game 22 Score 1 Record: 1 Epsilon: 59\n",
      "Game 23 Score 0 Record: 1 Epsilon: 58\n",
      "Game 24 Score 1 Record: 1 Epsilon: 57\n",
      "Game 25 Score 3 Record: 3 Epsilon: 56\n",
      "Game 26 Score 0 Record: 3 Epsilon: 55\n",
      "Game 27 Score 3 Record: 3 Epsilon: 54\n",
      "Game 28 Score 1 Record: 3 Epsilon: 53\n",
      "Game 29 Score 2 Record: 3 Epsilon: 52\n",
      "Game 30 Score 1 Record: 3 Epsilon: 51\n",
      "Game 31 Score 0 Record: 3 Epsilon: 50\n",
      "Game 32 Score 2 Record: 3 Epsilon: 49\n",
      "Game 33 Score 3 Record: 3 Epsilon: 48\n",
      "Game 34 Score 1 Record: 3 Epsilon: 47\n",
      "Game 35 Score 3 Record: 3 Epsilon: 46\n",
      "Game 36 Score 0 Record: 3 Epsilon: 45\n",
      "Game 37 Score 1 Record: 3 Epsilon: 44\n",
      "Game 38 Score 0 Record: 3 Epsilon: 43\n",
      "Game 39 Score 1 Record: 3 Epsilon: 42\n",
      "Game 40 Score 0 Record: 3 Epsilon: 41\n",
      "Game 41 Score 0 Record: 3 Epsilon: 40\n",
      "Game 42 Score 0 Record: 3 Epsilon: 39\n",
      "Game 43 Score 0 Record: 3 Epsilon: 38\n",
      "Game 44 Score 1 Record: 3 Epsilon: 37\n",
      "Game 45 Score 1 Record: 3 Epsilon: 36\n",
      "Game 46 Score 1 Record: 3 Epsilon: 35\n",
      "Game 47 Score 1 Record: 3 Epsilon: 34\n",
      "Game 48 Score 0 Record: 3 Epsilon: 33\n",
      "Game 49 Score 2 Record: 3 Epsilon: 32\n",
      "Game 50 Score 1 Record: 3 Epsilon: 31\n",
      "Game 51 Score 2 Record: 3 Epsilon: 30\n",
      "Game 52 Score 1 Record: 3 Epsilon: 29\n",
      "Game 53 Score 3 Record: 3 Epsilon: 28\n",
      "Game 54 Score 1 Record: 3 Epsilon: 27\n",
      "Game 55 Score 3 Record: 3 Epsilon: 26\n",
      "Game 56 Score 2 Record: 3 Epsilon: 25\n",
      "Game 57 Score 1 Record: 3 Epsilon: 24\n",
      "Game 58 Score 1 Record: 3 Epsilon: 23\n",
      "Game 59 Score 1 Record: 3 Epsilon: 22\n",
      "Game 60 Score 8 Record: 8 Epsilon: 21\n",
      "Game 61 Score 6 Record: 8 Epsilon: 20\n",
      "Game 62 Score 3 Record: 8 Epsilon: 19\n",
      "Game 63 Score 4 Record: 8 Epsilon: 18\n",
      "Game 64 Score 4 Record: 8 Epsilon: 17\n",
      "Game 65 Score 0 Record: 8 Epsilon: 16\n",
      "Game 66 Score 5 Record: 8 Epsilon: 15\n",
      "Game 67 Score 4 Record: 8 Epsilon: 14\n",
      "Game 68 Score 2 Record: 8 Epsilon: 13\n",
      "Game 69 Score 4 Record: 8 Epsilon: 12\n",
      "Game 70 Score 3 Record: 8 Epsilon: 11\n",
      "Game 71 Score 6 Record: 8 Epsilon: 10\n",
      "Game 72 Score 7 Record: 8 Epsilon: 9\n",
      "Game 73 Score 8 Record: 8 Epsilon: 8\n",
      "Game 74 Score 4 Record: 8 Epsilon: 7\n",
      "Game 75 Score 8 Record: 8 Epsilon: 6\n",
      "Game 76 Score 11 Record: 11 Epsilon: 5\n",
      "Game 77 Score 3 Record: 11 Epsilon: 4\n",
      "Game 78 Score 16 Record: 16 Epsilon: 3\n",
      "Game 79 Score 3 Record: 16 Epsilon: 2\n",
      "Game 80 Score 13 Record: 16 Epsilon: 1\n",
      "Game 81 Score 33 Record: 33 Epsilon: 0\n",
      "Game 82 Score 10 Record: 33 Epsilon: -1\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "display Surface quit",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 140\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[38;5;66;03m# plot_scores.append(score)\u001b[39;00m\n\u001b[0;32m    133\u001b[0m             \u001b[38;5;66;03m# total_score += score\u001b[39;00m\n\u001b[0;32m    134\u001b[0m             \u001b[38;5;66;03m# mean_score = total_score / agent.n_games\u001b[39;00m\n\u001b[0;32m    135\u001b[0m             \u001b[38;5;66;03m# plot_mean_scores.append(mean_score)\u001b[39;00m\n\u001b[0;32m    136\u001b[0m             \u001b[38;5;66;03m# plot(plot_scores, plot_mean_scores)\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 140\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 111\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    108\u001b[0m final_move \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_action(state_old)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m# perform move and get new state\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m reward, done, score \u001b[38;5;241m=\u001b[39m \u001b[43mgame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplay_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_move\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m state_new \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mget_state(game)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# train short memory\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 94\u001b[0m, in \u001b[0;36mSnakeGameAI.play_step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake\u001b[38;5;241m.\u001b[39mpop()\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# 5. update ui and clock\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_ui\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclock\u001b[38;5;241m.\u001b[39mtick(SPEED)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# 6. return game over and score\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[2], line 114\u001b[0m, in \u001b[0;36mSnakeGameAI._update_ui\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_ui\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBLACK\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pt \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnake:\n\u001b[0;32m    117\u001b[0m         pygame\u001b[38;5;241m.\u001b[39mdraw\u001b[38;5;241m.\u001b[39mrect(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisplay, BLUE1, pygame\u001b[38;5;241m.\u001b[39mRect(pt\u001b[38;5;241m.\u001b[39mx, pt\u001b[38;5;241m.\u001b[39my, BLOCK_SIZE, BLOCK_SIZE))\n",
      "\u001b[1;31merror\u001b[0m: display Surface quit"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "from collections import deque\n",
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 1000\n",
    "LR = 0.001\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.n_games = 0\n",
    "        self.epsilon = 0 # randomness\n",
    "        self.gamma = 0.9 # discount rate\n",
    "        self.memory = deque(maxlen=MAX_MEMORY) # popleft()\n",
    "        self.model = Linear_QNet(11, 256, 3)\n",
    "        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)\n",
    "\n",
    "\n",
    "    def get_state(self, game):\n",
    "        head = game.snake[0]\n",
    "        point_l = Point(head.x - 20, head.y)\n",
    "        point_r = Point(head.x + 20, head.y)\n",
    "        point_u = Point(head.x, head.y - 20)\n",
    "        point_d = Point(head.x, head.y + 20)\n",
    "        \n",
    "        dir_l = game.direction == Direction.LEFT\n",
    "        dir_r = game.direction == Direction.RIGHT\n",
    "        dir_u = game.direction == Direction.UP\n",
    "        dir_d = game.direction == Direction.DOWN\n",
    "\n",
    "        state = [\n",
    "            # Danger straight\n",
    "            (dir_r and game.is_collision(point_r)) or \n",
    "            (dir_l and game.is_collision(point_l)) or \n",
    "            (dir_u and game.is_collision(point_u)) or \n",
    "            (dir_d and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger right\n",
    "            (dir_u and game.is_collision(point_r)) or \n",
    "            (dir_d and game.is_collision(point_l)) or \n",
    "            (dir_l and game.is_collision(point_u)) or \n",
    "            (dir_r and game.is_collision(point_d)),\n",
    "\n",
    "            # Danger left\n",
    "            (dir_d and game.is_collision(point_r)) or \n",
    "            (dir_u and game.is_collision(point_l)) or \n",
    "            (dir_r and game.is_collision(point_u)) or \n",
    "            (dir_l and game.is_collision(point_d)),\n",
    "            \n",
    "            # Move direction\n",
    "            dir_l,\n",
    "            dir_r,\n",
    "            dir_u,\n",
    "            dir_d,\n",
    "            \n",
    "            # Food location \n",
    "            game.food.x < game.head.x,  # food left\n",
    "            game.food.x > game.head.x,  # food right\n",
    "            game.food.y < game.head.y,  # food up\n",
    "            game.food.y > game.head.y  # food down\n",
    "            ]\n",
    "\n",
    "        return np.array(state, dtype=int)\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done)) # popleft if MAX_MEMORY is reached\n",
    "\n",
    "    def train_long_memory(self):\n",
    "        if len(self.memory) > BATCH_SIZE:\n",
    "            mini_sample = random.sample(self.memory, BATCH_SIZE) # list of tuples\n",
    "        else:\n",
    "            mini_sample = self.memory\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*mini_sample)\n",
    "        self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def train_short_memory(self, state, action, reward, next_state, done):\n",
    "        self.trainer.train_step(state, action, reward, next_state, done)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        # random moves: tradeoff exploration / exploitation\n",
    "        self.epsilon = 80 - self.n_games\n",
    "        final_move = [0,0,0]\n",
    "        if random.randint(0, 200) < self.epsilon:\n",
    "            move = random.randint(0, 2)\n",
    "            final_move[move] = 1\n",
    "        else:\n",
    "            state0 = torch.tensor(state, dtype=torch.float)\n",
    "            prediction = self.model(state0)\n",
    "            move = torch.argmax(prediction).item()\n",
    "            final_move[move] = 1\n",
    "\n",
    "        return final_move\n",
    "\n",
    "\n",
    "def train():\n",
    "    record = 0\n",
    "    agent = Agent()\n",
    "    game = SnakeGameAI()\n",
    "    while True:\n",
    "        # get old state\n",
    "        state_old = agent.get_state(game)\n",
    "\n",
    "        # get move\n",
    "        final_move = agent.get_action(state_old)\n",
    "\n",
    "        # perform move and get new state\n",
    "        reward, done, score = game.play_step(final_move)\n",
    "        state_new = agent.get_state(game)\n",
    "\n",
    "        # train short memory\n",
    "        agent.train_short_memory(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        # remember\n",
    "        agent.remember(state_old, final_move, reward, state_new, done)\n",
    "\n",
    "        if done:\n",
    "            # train long memory, plot result\n",
    "            game.reset()\n",
    "            agent.n_games += 1\n",
    "            agent.train_long_memory()\n",
    "\n",
    "            if score > record:\n",
    "                record = score\n",
    "                agent.model.save()\n",
    "\n",
    "            print('Game', agent.n_games, 'Score', score, 'Record:', record,'Epsilon:' ,agent.epsilon)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
